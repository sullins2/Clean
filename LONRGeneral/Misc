

Plotting stuff:

# self.iters.append(t)
            #
            # if self.GRIDWORLD:
            #     if self.M.noise > 0:
            #         self.runningSum += self.M.Q[0][36][3]
            #         self.runningSumPI += self.M.pi[0][36][3]
            #     else:
            #         # NORTH
            #         self.runningSum += self.M.Q[0][36][0]
            #         self.runningSumPI += self.M.pi[0][36][0]
            #     #print("RUNPIS: ", self.runningSumPI)
            #     #for x in range(36):
            #     self.PIsums.append(float(self.runningSumPI) / float(t))
            #     self.QSums.append(float(self.runningSum) / float(t))
            #
            # elif self.NOSDE:
            #     # Player 0, state left, KEEP
            #     self.QSumSendRunningSum += self.M.Q[0][1]["SEND"]
            #     self.QSumSEND.append(float(self.QSumSendRunningSum) / float(t))
            #     # Player 0, state left
            #     self.QSumKEEPRunningSum += self.M.Q[0][1]["KEEP"]
            #     self.QSumKEEP.append(float(self.QSumKEEPRunningSum) / float(t))
            #
            #     self.QSumNOOPRunningSum += self.M.Q[0][2]["NOOP"]
            #     self.QSumNOOP.append(float(self.QSumNOOPRunningSum) / float(t))
            #
            #     self.Pi1SUMSENDSum += self.M.pi[0][1]["SEND"]
            #     self.Pi2SUMSENDSum += self.M.pi[1][2]["SEND"]
            #     self.Pi1SEND.append(float(self.Pi1SUMSENDSum) / float(t))
            #     self.Pi2SEND.append(float(self.Pi2SUMSENDSum) / float(t))
            #
            #     self.reg1Sum = self.M.regret_sums[0][1]["SEND"]
            #     self.reg2Sum = self.M.regret_sums[1][2]["SEND"]
            #     self.reg1.append(self.reg1Sum)
            #     self.reg2.append(self.reg2Sum)
            #
            # elif self.SOCCER:
            #     #print("VAL: ", self.M.Q[0]["B21"])
            #     self.QSumSoccerB21RightRunningSum += self.M.Q[0]["B21"][3]
            #     self.QSumSoccerB21Right.append(float(self.QSumSoccerB21RightRunningSum) / float(t))
            # tt = t

            # print("LastAVG: ", float(self.runningSum) / float(tt))
        # print("Avg: ", float(self.runningSum) / float(iterations))
        #print(self.QSumSEND)
        #print(self.QSums)
        #print("PRINTED QSUMS")
        # if self.GRIDWORLD:
        #     return self.QSums, self.PIsums, self.iters
        # elif self.NOSDE:
        #
        #     return self.QSumSEND, self.QSumKEEP, self.QSumNOOP, self.Pi1SEND, self.Pi2SEND, self.reg1, self.reg2, self.iters
        # elif self.SOCCER:
        #     return self.QSumSoccerB21Right, self.iters





                    if self.GRIDWORLD:
                if self.M.noise > 0:
                    self.runningSum += self.M.Q[0][36][3]
                else:
                    self.runningSum += self.M.Q[0][36][0]
                self.QSums.append(float(self.runningSum) / float(t))
            elif self.NOSDE:
                # Player 0, state left, SEND
                self.QSumSendRunningSum += self.M.Q[0][1]["SEND"]
                self.QSumSEND.append(float(self.QSumSendRunningSum) / float(t))
                # Player 0, state left, KEEP
                self.QSumKEEPRunningSum += self.M.Q[0][1]["KEEP"]
                self.QSumKEEP.append(float(self.QSumKEEPRunningSum) / float(t))

                # Player 0, state right NOOP
                self.QSumNOOPRunningSum += self.M.Q[0][2]["NOOP"]
                self.QSumNOOP.append(float(self.QSumNOOPRunningSum) / float(t))

                self.Pi1SUMSENDSum += self.M.pi[0][1]["SEND"]
                self.Pi2SUMSENDSum += self.M.pi[0][2]["SEND"]
                self.Pi1SEND.append(float(self.Pi1SUMSENDSum) / float(t))
                self.Pi2SEND.append(float(self.Pi2SUMSENDSum) / float(t))

            elif self.SOCCER:
                # print("VAL: ", self.M.Q[0]["B21"])
                self.QSumSoccerB21RightRunningSum += self.M.Q[0]["B21"][3]
                self.QSumSoccerB21Right.append(float(self.QSumSoccerB21RightRunningSum) / float(t))

            elif self.TIGERGAME:
                self.LRunningSum += self.M.Q[0]["rootTL"]["L"]
                self.L.append(float(self.LRunningSum) / float(t))
                self.OLRunningSum += self.M.Q[0]["rootTL"]["OL"]
                self.OL.append(float(self.OLRunningSum) / float(t))
                self.ORRunningSum += self.M.Q[0]["rootTL"]["OR"]
                self.OR.append(float(self.ORRunningSum) / float(t))

                self.rootTLLGLLGLRS += self.M.Q[0]["rootTLLGLLGL"]["OR"]
                self.rootTLLGLLGL.append(float(self.rootTLLGLLGLRS) / float(t))

                self.rootTLLGRLGRRS += self.M.Q[0]["rootTLLGRLGR"]["OL"]
                self.rootTLLGRLGR.append(float(self.rootTLLGRLGRRS) / float(t))

                self.rootTLLGLLGRRS += self.M.Q[0]["rootTLLGLLGR"]["L"]
                self.rootTLLGLLGR.append(float(self.rootTLLGLLGRRS) / float(t))

                self.rootTLLGRLGLRS += self.M.Q[0]["rootTLLGRLGL"]["L"]
                self.rootTLLGRLGL.append(float(self.rootTLLGRLGLRS) / float(t))

                self.baseORSum += self.M.pi[0]["rootTLLGLLGL"]["OR"]
                self.baseOLSum += self.M.pi[0]["rootTLLGRLGR"]["OL"]
                self.baseOR.append(float(self.baseORSum) / float(t))
                self.baseOL.append(float(self.baseOLSum) / float(t))



                self.iters.append(t)


                    # print("LastAVG: ", float(self.runningSum) / float(tt))
            # print("Avg: ", float(self.runningSum) / float(iterations))
            # print(self.QSumSEND)
            # print(self.QSums)
            # print("PRINTED QSUMS")
        if self.GRIDWORLD:
            return self.QSums, self.iters
        elif self.NOSDE:

            return self.QSumSEND, self.QSumKEEP, self.QSumNOOP, self.iters
        elif self.SOCCER:
            return self.QSumSoccerB21Right, self.iters

        elif self.TIGERGAME:
            # print("PISUMS:")
            # print("PISUM LGLLGL: ", self.M.pi_sums[0]["rootTLLGLLGL"])
            # print("PISUM LGRLGR: ", self.M.pi_sums[0]["rootTLLGRLGR"])
            # print("PISUM LGLLGR: ", self.M.pi_sums[0]["rootTLLGLLGR"])
            # print("PISUM LGRLGL: ", self.M.pi_sums[0]["rootTLLGRLGL"])
            # print("")
            # print("Q:")
            # print("PISUM LGLLGL: ", self.M.Q[0]["rootTLLGLLGL"])
            # print("PISUM LGRLGR: ", self.M.Q[0]["rootTLLGRLGR"])
            # print("PISUM LGLLGR: ", self.M.Q[0]["rootTLLGLLGR"])
            # print("PISUM LGRLGL: ", self.M.Q[0]["rootTLLGRLGL"])
            sdfs = 3
            # print("Final Q of GRGROL: ", self.rootTLLGRLGR[-1])

            return self.L, self.OL, self.OR, self.rootTLLGLLGL, self.rootTLLGRLGR, self.rootTLLGLLGR, self.rootTLLGRLGL, self.baseOR, self.baseOL, self.iters




            # GRIDWORLD
        # self.runningSum = 0.0
        # self.runningSumPI = 0.0
        # self.PIsums = []

        # NoSDE
        # self.QSumSEND = []
        # self.QSumSendRunningSum = 0.0
        # self.QSumKEEP = []
        # self.QSumKEEPRunningSum = 0.0
        # self.QSumNOOP = []
        # self.QSumNOOPRunningSum = 0.0
        # self.pisumSEND = []
        # self.pisumKEEP = []
        # self.pisumNOOP = []

        # self.Pi1SUMSENDSum = 0.0
        # self.Pi2SUMSENDSum = 0.0
        # self.Pi1SEND = []
        # self.Pi2SEND = []

        # self.reg1Sum = 0.0
        # self.reg2Sum = 0.0
        # self.reg1 = []
        # self.reg2 = []

        # SoccerGame A12
        # self.QSumSoccerB21Right = []
        # self.QSumSoccerB21RightRunningSum = 0.0

        # Tiger Game
        # self.L = []
        # self.LRunningSum = 0.0
        # self.OL = []
        # self.OLRunningSum = 0.0
        # self.OR = []
        # self.ORRunningSum = 0.0
        #
        # self.rootTLLGLLGL = []
        # self.rootTLLGLLGLRS = 0.0
        # self.rootTLLGRLGR = []
        # self.rootTLLGRLGRRS = 0.0
        # self.rootTLLGLLGR = []
        # self.rootTLLGLLGRRS = 0.0
        # self.rootTLLGRLGL = []
        # self.rootTLLGRLGLRS = 0.0
        #
        # self.baseORSum = 0.0
        # self.baseOLSum = 0.0
        # self.baseOR = []
        # self.baseOL = []


# Tiger Game
        # elif self.randomize == True:
        #     print("Tiger game - value iteration")
        #     r = np.random.randint(0, 2)
        #     #if r == 0:
        #     if self.alt == 1:
        #         WW = self.M.totalStatesLeft
        #         self.alt = 2
        #     else:
        #         WW = self.M.totalStatesRight
        #         self.alt = 1
        #
        #     print("WW: ")
        #     print(WW)
        #     for n in range(self.M.N):
        #
        #         # Loop through all states
        #         for s in WW: #self.M.getStates(): #WW:
        #
        #             print(" S in loop: ", s)
        #             # Loop through actions of current player n
        #             # for a in self.M.getActions(s, n):
        #             aa = self.M.getActions(s, n)
        #             # if self.randomize == False:
        #             #     self.QUpdate(n, s, a, randomS=None)
        #             # else:
        #             print("AA: ", aa)
        #             for a in aa:
        #                 #print(" a: ", a)
        #                 al = []
        #                 al.append(a)
        #                 print(" a: ", a)
        #                 self.QUpdate(n, s=s, a_current2=al, randomS=s)




















































        ##################################################################################################


          # def draw(self, weights):
    #
    #     # norm = sum(weights)
    #     # ww = []
    #     # for w in weights:
    #     #     ww.append(w / norm)
    #     # ra = np.random.choice([0,1,2,3], p=ww)
    #     #
    #     # return ra
    #     choice = np.random.uniform(0, sum(weights))
    #     choiceIndex = 0
    #
    #     for weight in weights:
    #         choice -= weight
    #         if choice <= 0:
    #             return choiceIndex
    #
    #         choiceIndex += 1
    #
    # def distr(self, weights, gamma=0.0):
    #     theSum = float(sum(weights))
    #     return list((1.0 - gamma) * (w / theSum) + (gamma / len(weights)) for w in weights)
    #
    # def distrForPISUMS(self, weights, gamma=0.0):
    #     theSum = float(sum(weights))
    #     return list((1.0 - gamma) * (w / theSum) for w in weights) # + (gamma / len(weights))
    #
    # # def mean(self, aList):
    # #     theSum = 0
    # #     count = 0
    # #
    # #     for x in aList:
    # #         theSum += x
    # #         count += 1
    # #
    # #     return 0 if count == 0 else theSum / count
    #
    #
    # def exp3Update(self, n, currentState, numActions, reward, gamma, t, rewardMin = -1.0, rewardMax = 10.0):
    #
    #
    #     if t % 2000 == 0:
    #         verbose = True
    #     else:
    #         verbose = False
    #     if verbose:
    #         print("Iteration: ", t)
    #         print("Current state: ", currentState)
    #
    #     weightsA = {}
    #
    #     # Get weights for actions [UP, RIGHT, DOWN, LEFT]
    #     for a in sorted(self.M.weights[n][currentState].keys()):
    #         weightsA[a] = self.M.weights[n][currentState][a]
    #
    #     # Put weights into a list
    #     weights = []
    #     for a in weightsA.keys():
    #         weights.append(weightsA[a])
    #
    #     if verbose: print("Weights: ", weights)
    #
    #
    #
    #     gamma = self.exp3gamma
    #     # Get distribution
    #     probabilityDistribution = self.distr(weights, gamma)
    #
    #     probDistForPISUMS = self.distrForPISUMS(weights, gamma=0.0)
    #
    #     # Loop thru and set pi
    #     aa = 0
    #     for a in sorted(self.M.weights[0][currentState].keys()):
    #         self.M.pi[0][currentState][a] = probabilityDistribution[aa]
    #         self.M.pi_sums[0][currentState][a] += probDistForPISUMS[aa]
    #         aa += 1
    #
    #     # Get action choice from prob dist
    #     if verbose: print("ProbDist: ", probabilityDistribution)
    #     choice = self.draw(probabilityDistribution)
    #
    #     # Get reward (Q-Value)
    #     if verbose: print("Choice: ", choice)  #0-3
    #     theReward = self.M.Q[0][currentState][choice]
    #
    #     # Scale reward (Not sure this is correct, scaling is set in function arguments above) line 386
    #     if verbose: print("Reward for choice: ", theReward)
    #     scaledReward = (theReward - rewardMin) / (rewardMax - rewardMin)  # rewards scaled to 0,1
    #
    #     # Not entirely sure what this does
    #     if verbose: print("Scaled Reward: ", scaledReward)
    #     estimatedReward = 1.0 * scaledReward / probabilityDistribution[choice]
    #     if verbose: print("Estimated reward: ", estimatedReward)
    #
    #     # Update weights
    #     if verbose: print("Updating choice: ", choice)
    #
    #     #if verbose: print("Weights after: ", self.M.weights[0][currentState])
    #     self.M.weights[0][currentState][choice] *= math.exp(estimatedReward * gamma / float(numActions))  # important that we use estimated reward here!
    #
    #     # minW = self.M.weights[0][currentState][0]
    #     # # if verbose: print("Init min weight: ", self.M.weights[0][currentState][0])
    #     # aa = 0
    #     # for w in self.M.weights[0][currentState].keys():
    #     #     if self.M.weights[0][currentState][w] < minW:
    #     #         minW = self.M.weights[0][currentState][w]
    #     #     aa += 1
    #
    #     # if t % 100 == 0:
    #     #     for w in self.M.weights[0][currentState].keys():
    #     #         #if t < 120: continue
    #     #         # if t % 100 == 0:
    #     #         self.M.weights[0][currentState][w] = self.M.weights[0][currentState][w] - (minW / 1.0) + 1.0
    #     #         # if self.M.weights[0][currentState][w] < 1:
    #     #         #     self.M.weights[0][currentState][w] = 1.0
    #     if verbose: print("")
    #
    #     # Return action
    #     return choice

    # def QLearningPIUpdate(self, n, currentState, t):

######################################################################################

















            # Loop through all actions
            # for a in self.M.getActions(currentState, 0):
            #
            #     # Get successor states
            #     succs = self.M.getNextStatesAndProbs(currentState, a, 0)
            #
            #     Value = 0.0
            #     # Loop through each s', T(s' | s, a)
            #     #   - Last parameter in loop is reward, which eventually will be included
            #     #       -
            #     for s_prime, prob, _ in succs:
            #
            #         tempValue = 0.0
            #         for a_prime in self.M.getActions(s_prime, 0):
            #             tempValue += self.M.Q[n][self.M.getStateRep(s_prime)][a_prime] * self.M.pi[n][self.M.getStateRep(s_prime)][a_prime]
            #         Value += prob * (self.M.getReward(currentState, a, 0,0) + self.gamma * tempValue)
            #
            #         if s_prime not in totStates:
            #             totStates.append(s_prime)
            #
            #     self.M.Q_bu[n][self.M.getStateRep(currentState)][a] = (1.0 - self.alpha) * self.M.Q[n][self.M.getStateRep(currentState)][a] + self.alpha * Value


            # Copy Q Values over for states visited
            # for s in totStates:
            #     for a in self.M.getActions(s, 0):
            #         self.M.Q[n][self.M.getStateRep(s)][a] = self.M.Q_bu[n][self.M.getStateRep(s)][a]
            #         # self.Qsums[self.tigergame.getStateRep(s)][a] += self.Q_bu[self.tigergame.getStateRep(s)][a]

###################################################################################

# Calculate regret - this variable name needs a better name
            # target = 0.0
            #
            # for a in self.M.getActions(currentState, 0):
            #     target += self.M.Q[n][self.M.getStateRep(currentState)][a] * self.M.pi[n][self.M.getStateRep(currentState)][a]
            #
            # for a in self.M.getActions(currentState, 0):
            #     action_regret = self.M.Q[n][self.M.getStateRep(currentState)][a] - target
            #
            #     #RMPLUS = True
            #     if self.RMPLUS:
            #         self.M.regret_sums[n][self.M.getStateRep(currentState)][a] = max(0.0, self.M.regret_sums[n][self.M.getStateRep(currentState)][a] + action_regret)
            #     else:
            #         self.M.regret_sums[n][self.M.getStateRep(currentState)][a] += action_regret
            #
            #
            # # Skip terminal states
            # # if self.M.isTerminal(self.M.getStateRep(currentState)) == True:
            # #     continue
            #
            # for a in self.M.getActions(self.M.getStateRep(currentState), 0):
            #
            #     # Sum up total regret
            #     rgrt_sum = 0.0
            #     for k in self.M.regret_sums[n][self.M.getStateRep(currentState)].keys():
            #         rgrt_sum += self.M.regret_sums[n][self.M.getStateRep(currentState)][k] if self.M.regret_sums[n][self.M.getStateRep(currentState)][k] > 0 else 0.0
            #
            #     # Check if this is a "trick"
            #     # Check if this can go here or not
            #     if rgrt_sum > 0:
            #         self.M.pi[n][self.M.getStateRep(currentState)][a] = (max(self.M.regret_sums[n][self.M.getStateRep(currentState)][a], 0.)) / rgrt_sum
            #     else:
            #         self.M.pi[n][self.M.getStateRep(currentState)][a] = 1.0 / len(self.M.getActions(self.M.getStateRep(currentState),0))
            #
            #     # Add to policy sum
            #     self.M.pi_sums[n][self.M.getStateRep(currentState)][a] += self.M.pi[n][self.M.getStateRep(currentState)][a]


#####################################################################################################




# if self.M.isTerminal(s): return True
        # if self.M.isTerminal(s):
        #     self.M.Q_bu[n][s][a_current] = (1.0 - self.alpha) * self.M.Q[n][s][a_current] + self.alpha * self.M.getReward(s, a_current, 0, 0)
        #     continue
        #
        # # Get next states and transition probs for MDP
        # # Get next states and other players policy for MG
        # succs = self.M.getNextStatesAndProbs(s, a_current, n)
        # # print("s: ", s, " a:", a_current, "   succs: ", succs)
        #
        # # print("  SUCCS: ", succs)
        # Value = 0.0
        #
        # # Loop thru each next state and prob
        # for s_prime, prob, reward in succs:
        #     # print(" - s_prime: ", s_prime, "  prob: ", prob, "  reward: ", reward)
        #     tempValue = 0.0
        #
        #     # Loop thru actions in s_prime for player n
        #     for a_current_prime in self.M.getActions(s_prime, n):
        #         tempValue += self.M.Q[n][s_prime][a_current_prime] * self.M.pi[n][s_prime][a_current_prime]
        #
        #     Value += prob * (reward + self.gamma * tempValue)
        #
        # self.M.Q_bu[n][s][a_current] = (1.0 - self.alpha) * self.M.Q[n][s][a_current] + (self.alpha) * Value

        # print("Entered QUpdate: ", " s: ", currentState, " a:", a_current)
        # if self.M.isTerminal(self.M.getStateRep(currentState)) == True:
        #     self.M.Q_bu[n][self.M.getStateRep(currentState)][a_current] = (1.0 - self.alpha) * self.M.Q[n][self.M.getStateRep(currentState)][a_current] + self.alpha * self.M.getReward(self.M.getStateRep(currentState), a_current, 0, 0)
        #     #for a in self.M.getActions(currentState, n):
        #         # Note, the reward is via the actual state, so there is no getStateRep()
        #     #    self.M.Q_bu[n][self.M.getStateRep(currentState)][a] = (1.0 - self.alpha) * self.M.Q[n][self.M.getStateRep(currentState)][a] + self.alpha * self.M.getReward(currentState, a, a, a)
        #     return
        #     # done = True
        #     # continue
        #
        # # totStates keeps track of which states need Qvalue copying
        # #   as not all states need to be backed up, only the ones visited
        # totStates = []
        # totStates.append(currentState)
        #
        # # Loop through all actions
        # for a in self.M.getActions(currentState, n):
        #     # print(" - in loop a: ", a)
        #     # Get successor states
        #     succs = self.M.getNextStatesAndProbs(currentState, a, n)
        #
        #     Value = 0.0
        #     # Loop through each s', T(s' | s, a)
        #     #   - Last parameter in loop is reward, which eventually will be included
        #     #       -
        #     for s_prime, prob, _ in succs:
        #
        #         tempValue = 0.0
        #         for a_prime in self.M.getActions(s_prime, n):
        #             # print("  -- SR: ", self.M.getStateRep(s_prime))
        #             # print("    - ", self.M.Q[n][self.M.getStateRep(s_prime)]) #self.M.pi[n][self.M.getStateRep(s_prime)][a_prime])
        #             tempValue += self.M.Q[n][self.M.getStateRep(s_prime)][a_prime] * self.M.pi[n][self.M.getStateRep(s_prime)][a_prime]
        #         Value += prob * (self.M.getReward(self.M.getStateRep(currentState), a, 0, 0) + self.gamma * tempValue)
        #
        #         # REMOVE: Q[S_PRIME] is not updated/changed
        #         if s_prime not in totStates:
        #             totStates.append(s_prime)
        #
        #     self.M.Q_bu[n][self.M.getStateRep(currentState)][a_current] = (1.0 - self.alpha) * self.M.Q[n][self.M.getStateRep(currentState)][a_current] + self.alpha * Value

        # Copy Q Values over for states visited
        # for s in totStates:
        #     for a in self.M.getActions(s, n):
        #         self.M.Q[n][self.M.getStateRep(s)][a] = self.M.Q_bu[n][self.M.getStateRep(s)][a]
                # self.Qsums[self.tigergame.getStateRep(s)][a] += self.Q_bu[self.tigergame.getStateRep(s)][a]

        # Don't update terminal states
        # if self.M.isTerminal(self.M.getStateRep(currentState)) == True:
        #     continue

#############################################################################


 #             if self.M.isTerminal(s):
        #                 self.M.Q_bu[n][s][a_current] = (1.0 - self.alpha)*self.M.Q[n][s][a_current] + self.alpha*self.M.getReward(s, a_current, 0,0)
        #                 continue
        #
        #             # Get next states and transition probs for MDP
        #             # Get next states and other players policy for MG
        #             succs = self.M.getNextStatesAndProbs(s, a_current, n)
        #             #print("s: ", s, " a:", a_current, "   succs: ", succs)
        #
        #             #print("  SUCCS: ", succs)
        #             Value = 0.0
        #
        #             #Loop thru each next state and prob
        #             for s_prime, prob, reward in succs:
        #                 #print(" - s_prime: ", s_prime, "  prob: ", prob, "  reward: ", reward)
        #                 tempValue = 0.0
        #
        #                 # Loop thru actions in s_prime for player n
        #                 for a_current_prime in self.M.getActions(s_prime, n):
        #                     tempValue += self.M.Q[n][s_prime][a_current_prime] * self.M.pi[n][s_prime][a_current_prime]
        #
        #                 Value += prob * (reward + self.gamma * tempValue)
        #
        #             self.M.Q_bu[n][s][a_current] = (1.0 - self.alpha)*self.M.Q[n][s][a_current] + (self.alpha)*Value
        #
        #
        # ####################################################################################################
        #
        # Back up Q
        # for n in range(self.M.N):
        #     for s in self.M.getStates():
        #         for a in self.M.getActions(s, n):
        #             self.M.Q[n][s][a] = self.M.Q_bu[n][s][a]
        #             self.M.QSums[n][s][a] += self.M.Q[n][s][a]

        # Regrets / policy updates, etc
        # For each player

        # iters = t + 1
        # alphaR = 3.0 / 2.0  # accum pos regrets
        # betaR = 0.0         # accum neg regrets
        # gammaR = 2.0        # contribution to avg strategy
        #
        # alphaW = pow(iters, alphaR)
        # alphaWeight = (alphaW / (alphaW + 1))
        #
        # betaW = pow(iters, betaR)
        # betaWeight = (betaW / (betaW + 1))
        #
        # gammaWeight = pow((iters / (iters + 1)), gammaR)
        #
        # for n in range(self.M.N):
        #
        #     # For each state
        #     for s in self.M.getStates():
        #
        #         # Skip terminals
        #         if self.M.isTerminal(s): continue
        #
        #         target = 0.0
        #         for a in self.M.getActions(s, n):
        #             target += self.M.Q[n][s][a] * self.M.pi[n][s][a]
        #
        #         for a in self.M.getActions(s, n):
        #             action_regret = self.M.Q[n][s][a] - target
        #
        #             if self.DCFR:
        #                 if action_regret > 0:
        #                     action_regret *= alphaWeight
        #                 else:
        #                     action_regret *= betaWeight
        #
        #             self.M.regret_sums[n][s][a] += action_regret
        #             #self.M.regret_sums[n][s][a] = max(0.0, self.M.regret_sums[n][s][a] + action_regret)
        #
        #     for s in self.M.getStates():
        #
        #         # Skip terminals
        #         if self.M.isTerminal(s): continue
        #
        #         rgrt_sum = 0.0
        #         for a in self.M.getActions(s, n):
        #             rgrt_sum += max(0.0, self.M.regret_sums[n][s][a])
        #
        #         for a in self.M.getActions(s, n):
        #             if rgrt_sum > 0:
        #                 self.M.pi[n][s][a] = (max(self.M.regret_sums[n][s][a], 0.0) / rgrt_sum)
        #             else:
        #                 self.M.pi[n][s][a] = 1. / len(self.M.getActions(s, n))
        #
        #             if self.DCFR:
        #                 self.M.pi_sums[n][s][a] += self.M.pi[n][s][a] * gammaWeight
        #             else:
        #                 self.M.pi_sums[n][s][a] += self.M.pi[n][s][a]









##################################




#
# import numpy as np
#
#
# class MDP(object):
#
#     def __init__(self, startState=None):
#
#         # Check if these are used
#         self.totalPlayers = 1
#         self.totalStates = 1
#
#         # Initialized here but must be defined/created in created class
#         self.Q = None
#         self.Q_bu = None
#         self.QSums = None
#         self.pi = None
#         self.pi_sums = None
#         self.regret_sums = None
#
#         self.startState = startState
#
#     def getActions(self, s, n):
#         """ Returns list of actions
#
#         """
#         raise NotImplementedError("Please implement information_set method")
#
#     def getNextStatesAndProbs(self, s, a_current, n_current):
#         """ Returns list of [ [next_state, prob, rew] ]
#
#         """
#         raise NotImplementedError("Please implement information_set method")
#
#
#     def getReward(self, s, a_current, n, a_notN):
#         """ s=state, a_current=actionOfPlayer n, n is current player, a_notN is action of other player
#             Joint Actions required for Markov Games
#             a_notN can be set to 0 for MDPs and ignored
#
#         Returns reward(s, a, a_other)
#
#         """
#         raise NotImplementedError("Please implement information_set method")
#
#     def getStates(self):
#         """Return List of States
#
#         """
#         raise NotImplementedError("Please implement information_set method")
#
#     def isTerminal(self, s):
#         raise NotImplementedError("Please implement information_set method")
#
#
#     # The following are only needed for O-LONR
#
#     def getMove(self, s, a):
#         """Used to get actual move in the environment for O-LONR
#         """
#         raise NotImplementedError("Please implement information_set method")
#
#     def getStateRep(self, s):
#         """ O-LONR use only
#
#             For MDPs: simply returns state
#             For TigerGame: Maps both to one (EX: TLL & TRL -> TLL) aka one set of Q Values
#         """
#         raise NotImplementedError("Please implement information_set method")
#
# class LONR(object):
#
#     # M: Markov game (MDP, markov game, tiger game)
#
#     def __init__(self, M=None, gamma=1.0, alpha=1.0, epsilon=10, alphaDecay=1.0, DCFR=False):
#         self.M = M
#         #self.N = M.totalPlayers
#         #self.S = M.totalStates
#
#         self.gamma = gamma
#         self.alpha = alpha
#         self.epsilon = epsilon
#         self.alphaDecay = alphaDecay
#
#
#         self.DCFR = DCFR
#
#     def lonr_value_iteration(self, iterations=-1, log=-1):
#
#         print("Starting training..")
#         for t in range(1, iterations+1):
#
#             if (t+1) % log == 0:
#                 print("Iteration: ", t+1, " alpha: ", self.alpha)
#
#             self._lonr_value_iteration(t=t)
#             #print("")
#             self.alpha *= self.alphaDecay
#             self.alpha = max(0.0, self.alpha)
#
#         print("Finish Training")
#
#     def _lonr_value_iteration(self, t):
#
#         # Loop through all players
#         for n in range(self.M.N):
#             #print("N: ", n)
#
#             # Loop through all states
#             for s in self.M.getStates():
#                 #print(" s: ", s)
#
#                 # Loop through actions of current player n
#                 for a_current in self.M.getActions(s, n):
#                     #print("  a_current: ", a_current)
#
#                     if self.M.isTerminal(s):
#                         # This is here because terminals have actions like "exit"
#                         # to collect the reward. To make the code easier, the terminals
#                         # have the same actions as everywhere else, but each action is an "exit"
#                         #
#                         # This was a remnant of when I was using numpy arrays and couldn't set the size
#                         # for one index different than the rest easily (Q[totalStates][totalActions] for
#                         # instance, but Q[terminals][oneExitAction]
#                         #
#                         # But now the MDP class handles that, and if I change them all to dicts/maps
#                         # and not numpy arrays, I wont need this special case here.
#                         #
#                         # This works out below if/when an expectation is taken because the initial
#                         # uniform policy never changes so the expected value = reward
#                         #
#                         # I might possibly change it so that instead of setting it here,
#                         # M.getActions(terminal) returns one exit action
#                         self.M.Q_bu[n][s][a_current] = (1.0 - self.alpha)*self.M.Q[n][s][a_current] * + self.alpha*self.M.getReward(s, a_current, 0,0)
#                         #print("Terminal: ", s)
#                         continue
#
#                     # Get next states and transition probs for MDP
#                     # Get next states and other players policy for MG
#                     succs = self.M.getNextStatesAndProbs(s, a_current, n)
#                     #print("s: ", s, " a:", a_current, "   succs: ", succs)
#
#
#                     Value = 0.0
#
#                     #Loop thru each next state and prob
#                     for s_prime, prob, reward in succs:
#                         #print(" - s_prime: ", s_prime, "  prob: ", prob, "  reward: ", reward)
#                         tempValue = 0.0
#
#                         # Loop thru actions in s_prime for player n
#                         for a_current_prime in self.M.getActions(s_prime, n):
#                             tempValue += self.M.Q[n][s_prime][a_current_prime] * self.M.pi[n][s_prime][a_current_prime]
#
#                         Value += prob * (reward + self.gamma * tempValue)
#
#                     self.M.Q_bu[n][s][a_current] = (1.0 - self.alpha)*self.M.Q[n][s][a_current] + (self.alpha)*Value
#
#
#         ####################################################################################################
#
#         # Back up Q
#         for n in range(self.M.N):
#             for s in self.M.getStates():
#                 for a in self.M.getActions(s, n):
#                     self.M.Q[n][s][a] = self.M.Q_bu[n][s][a]
#                     self.M.QSums[n][s][a] += self.M.Q[n][s][a]
#
#         # Regrets / policy updates, etc
#         # For each player
#
#         iters = t + 1
#         alphaR = 3.0 / 2.0  # accum pos regrets
#         betaR = 0.0         # accum neg regrets
#         gammaR = 2.0        # contribution to avg strategy
#
#         alphaW = pow(iters, alphaR)
#         alphaWeight = (alphaW / (alphaW + 1))
#
#         betaW = pow(iters, betaR)
#         betaWeight = (betaW / (betaW + 1))
#
#         gammaWeight = pow((iters / (iters + 1)), gammaR)
#
#         # Update regret sums
#         for n in range(self.M.N):
#
#             # For each state
#             for s in self.M.getStates():
#
#                 # Skip terminals
#                 if self.M.isTerminal(s): continue
#
#                 target = 0.0
#                 for a in self.M.getActions(s, n):
#                     target += self.M.Q[n][s][a] * self.M.pi[n][s][a]
#
#                 for a in self.M.getActions(s, n):
#                     action_regret = self.M.Q[n][s][a] - target
#
#                     if self.DCFR:
#                         if action_regret > 0:
#                             action_regret *= alphaWeight
#                         else:
#                             action_regret *= betaWeight
#
#                     self.M.regret_sums[n][s][a] += action_regret
#                     #self.M.regret_sums[n][s][a] = max(0.0, self.M.regret_sums[n][s][a] + action_regret)
#
#             # Update pi and pi sums
#             for s in self.M.getStates():
#
#                 # Skip terminals
#                 if self.M.isTerminal(s): continue
#
#                 rgrt_sum = 0.0
#                 for a in self.M.getActions(s, n):
#                     rgrt_sum += max(0.0, self.M.regret_sums[n][s][a])
#
#                 for a in self.M.getActions(s, n):
#                     if rgrt_sum > 0:
#                         self.M.pi[n][s][a] = (max(self.M.regret_sums[n][s][a], 0.0) / rgrt_sum)
#                     else:
#                         self.M.pi[n][s][a] = 1. / len(self.M.getActions(s, n))
#
#                     if self.DCFR:
#                         self.M.pi_sums[n][s][a] += self.M.pi[n][s][a] * gammaWeight
#                     else:
#                         self.M.pi_sums[n][s][a] += self.M.pi[n][s][a]
#
#
#     def lonr_online(self, iterations=-1, log=-1, randomized=False):
#
#         print("Starting training..")
#         for t in range(1, iterations+1):
#
#             if (t+1) % log == 0:
#                 print("Iteration: ", t+1, " alpha:", self.alpha)
#
#             self._lonr_online(t=t, totalIterations=iterations, randomized=randomized)
#
#
#
#             self.alpha *= self.alphaDecay
#             self.alpha = max(0.0, self.alpha)
#
#         print("Finish Training")
#
#
#     def _lonr_online(self, t=0, totalIterations=-1, randomized=False):
#         """
#
#         :param t: current iteration
#         :param totalIterations: total iterations
#         :param randomized: Randomizes the start state.
#                         True: For Tiger Game, to switch between TigerOnLeft, TigerOnRight
#                         False: For GridWorld, only one start state.
#         :return:
#         """
#
#
#         # For Tiger Game, randomize the MDP it sees
#         if randomized:
#
#             # Get the top root
#             startStates = self.M.getNextStatesAndProbs(self.M.startState, None, 0)
#
#             totalStartStates = []
#             totalStartStateProbs = []
#
#             # Pick Tiger on Left/Right based on probability set in M
#             for nextState, nextStateProb, _ in startStates:
#                 totalStartStates.append(nextState)
#                 totalStartStateProbs.append(nextStateProb)
#
#             # Randomly pick TigerOnLeft/Right based on M.TLProb (1-M.TLProb = TRProb)
#             currentState = np.random.choice(totalStartStates, p=totalStartStateProbs)
#
#         # For GridWorld, set currentState to the startState
#         else:
#             currentState = self.M.startState
#
#
#         done = False
#         n = 0  # One player
#
#         # Episode loop - until terminal state is reached
#         while done == False:
#
#             # Terminal - Set Q as Reward (ALL terminals have one action - "exit" which receives reward)
#             # Note:
#             #       GridWorld Terminals have the same 4 actions.
#             #           All get set here to the full reward
#             #           pi for each action remains 1 / totalActions
#             #       I intend to remove this part and include terminal rewards below in successors
#             #       But for now, in the expectation taken below, the value is correct:
#             #           For example:
#             #               Q[term] = (1/4)Reward + (1/4)Reward + (1/4)Reward + (1/4)Reward = Reward
#             #
#             #       Also, this backup is split via self.alpha.
#             #           For GridWorld, this does nothing (self.alpha = 1.0, removes the first part)
#             #           For Tiger Game, this allows shared Q's to converge and not be overwritten
#             if self.M.isTerminal(currentState) == True:
#                 for a in self.M.getActions(currentState, 0):
#                     # Note, the reward is via the actual state, so there is no getStateRep()
#                     self.M.Q_bu[n][self.M.getStateRep(currentState)][a] = (1.0 - self.alpha) * self.M.Q[n][self.M.getStateRep(currentState)][a] + self.alpha * self.M.getReward(currentState,a,a,a)
#                 done = True
#                 continue
#
#
#             # totStates keeps track of which states need Qvalue copying
#             #   as not all states need to be backed up, only the ones visited
#             totStates = []
#             totStates.append(currentState)
#
#             # Loop through all actions
#             for a in self.M.getActions(currentState, 0):
#
#                 # Get successor states
#                 succs = self.M.getNextStatesAndProbs(currentState, a, 0)
#
#                 Value = 0.0
#                 # Loop through each s', T(s' | s, a)
#                 #   - Last parameter in loop is reward, which eventually will be included
#                 #       -
#                 for s_prime, prob, _ in succs:
#
#                     tempValue = 0.0
#                     for a_prime in self.M.getActions(s_prime, 0):
#                         tempValue += self.M.Q[n][self.M.getStateRep(s_prime)][a_prime] * self.M.pi[n][self.M.getStateRep(s_prime)][a_prime]
#                     Value += prob * (self.M.getReward(currentState, a, 0,0) + self.gamma * tempValue)
#
#                     if s_prime not in totStates:
#                         totStates.append(s_prime)
#
#                 self.M.Q_bu[n][self.M.getStateRep(currentState)][a] = (1.0 - self.alpha) * self.M.Q[n][self.M.getStateRep(currentState)][a] + self.alpha * Value
#
#
#             # if max(list(self.M.Q[0][36].values())) -  max(list(self.M.Q_bu[0][36].values())) != 0:
#             #     print("Difference: ", max(list(self.M.Q[0][36].values())) -  max(list(self.M.Q_bu[0][36].values())))
#
#             # Copy Q Values over for states visited
#             for s in totStates:
#                 for a in self.M.getActions(s, 0):
#                     self.M.Q[n][self.M.getStateRep(s)][a] = self.M.Q_bu[n][self.M.getStateRep(s)][a]
#                     # self.Qsums[self.tigergame.getStateRep(s)][a] += self.Q_bu[self.tigergame.getStateRep(s)][a]
#
#
#             # Don't update terminal states
#             # if self.M.isTerminal(self.M.getStateRep(currentState)) == True:
#             #     continue
#
#             # Calculate regret - this variable name needs a better name
#             target = 0.0
#
#             for a in self.M.getActions(currentState, 0):
#                 target += self.M.Q[n][self.M.getStateRep(currentState)][a] * self.M.pi[n][self.M.getStateRep(currentState)][a]
#
#             for a in self.M.getActions(currentState, 0):
#                 action_regret = self.M.Q[n][self.M.getStateRep(currentState)][a] - target
#
#                 RMPLUS = True
#                 if RMPLUS:
#                     self.M.regret_sums[n][self.M.getStateRep(currentState)][a] = max(0.0, self.M.regret_sums[n][self.M.getStateRep(currentState)][a] + action_regret)
#                 else:
#                     self.M.regret_sums[n][self.M.getStateRep(currentState)][a] += action_regret
#
#
#             # Skip terminal states
#             # if self.M.isTerminal(self.M.getStateRep(currentState)) == True:
#             #     continue
#
#             for a in self.M.getActions(self.M.getStateRep(currentState), 0):
#
#                 # Sum up total regret
#                 rgrt_sum = 0.0
#                 for k in self.M.regret_sums[n][self.M.getStateRep(currentState)].keys():
#                     rgrt_sum += self.M.regret_sums[n][self.M.getStateRep(currentState)][k] if self.M.regret_sums[n][self.M.getStateRep(currentState)][k] > 0 else 0.0
#
#                 # Check if this is a "trick"
#                 # Check if this can go here or not
#                 if rgrt_sum > 0:
#                     self.M.pi[n][self.M.getStateRep(currentState)][a] = (max(self.M.regret_sums[n][self.M.getStateRep(currentState)][a], 0.)) / rgrt_sum
#                 else:
#                     self.M.pi[n][self.M.getStateRep(currentState)][a] = 1.0 / len(self.M.getActions(self.M.getStateRep(currentState),0))
#
#                 # Add to policy sum
#                 self.M.pi_sums[n][self.M.getStateRep(currentState)][a] += self.M.pi[n][self.M.getStateRep(currentState)][a]
#
#             # Epsilon Greedy action selection
#             if np.random.randint(0, 100) < int(self.epsilon):
#                 totalActions = self.M.getActions(currentState, 0)
#                 randomAction = np.random.randint(0, len(totalActions))
#                 randomAction = totalActions[randomAction]
#
#             else:
#
#                 totalActions = []
#                 totalActionsProbs = []
#                 ta = self.M.getActions(currentState, 0)
#                 for action in ta:
#                     totalActions.append(action)
#                     totalActionsProbs.append(self.M.pi[n][self.M.getStateRep(currentState)][action])
#                 #print(totalActionsProbs)
#                 randomAction = np.random.choice(totalActions, p=totalActionsProbs)
#
#             # randomAction picked, now simulate taking action
#             #       GridWorld: This will handle non-determinism, if there is non-determinism
#             #       TigerGame: This will either get the one next state OR
#             #                       if action is LISTEN, it will return next state based on
#             #                       observation accuracy aka (85/15) or (15/85), which is
#             #                       equivalent to hearing a growl left or growl right
#             nextPossStates = self.M.getNextStatesAndProbs(currentState, randomAction, 0)
#
#             # If there is only one successor state, pick that
#             if len(nextPossStates) == 1:
#                 # nextPossStates is list of lists
#                 # nextPossStates = [[next_state, prob, reward]]
#                 currentState = nextPossStates[0][0]
#
#             # More than one possible successor, pick via probabilities
#             #       GridWorld non-determ:
#             #           3 states: 1-self.noise, (1-self.noise)/2, (1-self.noise)/2
#             #               ex: 0.8, 0.1, 0.1 for randomAction, to the side, to the side
#             #
#             #       Tiger Game:
#             #           Only happens when randomAction is Listen
#             #               Return is based on which side tiger is on, and the obs accuracy
#             else:
#                 nextStates = []
#                 nextStateProbs = []
#                 for ns, nsp,_ in nextPossStates:
#                     nextStates.append(ns)
#                     nextStateProbs.append(nsp)
#                 currentState = np.random.choice(nextStates, p=nextStateProbs)




##########################################################################################

# NOSDE LP
# A = [[0, 1.5], [3, 1], [1, 0], [1, 1]]
# def ce(A, solver=None):
#     num_vars = len(A)
#     # maximize matrix c
#     c = [sum(i) for i in A] # sum of payoffs for both players
#     c = np.array(c, dtype="float")
#     c = matrix(c)
#     c *= -1 # cvxopt minimizes so *-1 to maximize
#     # constraints G*x <= h
#     G = build_ce_constraints(A=A)
#     G = np.vstack([G, np.eye(num_vars) * -1]) # > 0 constraint for all vars
#     h_size = len(G)
#     G = matrix(G)
#     h = [0 for i in range(h_size)]
#     h = np.array(h, dtype="float")
#     h = matrix(h)
#     # contraints Ax = b
#     A = [1 for i in range(num_vars)]
#     A = np.matrix(A, dtype="float")
#     A = matrix(A)
#     b = np.matrix(1, dtype="float")
#     b = matrix(b)
#     sol = solvers.lp(c=c, G=G, h=h, A=A, b=b, solver=solver)
#     return sol
#
# def build_ce_constraints(A):
#     num_vars = int(len(A) ** (1/2))
#     G = []
#     # row player
#     for i in range(num_vars): # action row i
#         for j in range(num_vars): # action row j
#             if i != j:
#                 constraints = [0 for i in A]
#                 base_idx = i * num_vars
#                 comp_idx = j * num_vars
#                 for k in range(num_vars):
#                     constraints[base_idx+k] = (- A[base_idx+k][0] + A[comp_idx+k][0])
#                 G += [constraints]
#     # col player
#     for i in range(num_vars): # action column i
#         for j in range(num_vars): # action column j
#             if i != j:
#                 constraints = [0 for i in A]
#                 for k in range(num_vars):
#                     constraints[i + (k * num_vars)] = (
#                         - A[i + (k * num_vars)][1]
#                         + A[j + (k * num_vars)][1])
#                 G += [constraints]
#     return np.matrix(G, dtype="float")
#
# sol = ce(A=A, solver="glpk")
# probs = sol["x"]
# print(probs)

################################################################################





















# for s in self.total_states.keys():
#
#     if self.isTerminal(s): continue
#
#     player_with_ball = s[0]
#
#     pAball = False
#     pBball = False
#     if player_with_ball == "A":
#         pAball = True
#     if player_with_ball == "B":
#         pBball = True
#
#     playerApos = int(s[1])
#     playerBpos = int(s[2])
#
#     pAx, pAy = stateToXY(playerApos)
#     pBx, pBy = stateToXY(playerBpos)
#
#     for actions_A in self.total_actions:
#         QV = 0.0
#         oppValue = 0.0
#         for actions_B in self.total_actions:
#
#             self.counter += 1
#
#             player_a = Player(x=pAx, y=pAy, has_ball=pAball, p_id='A')
#             player_b = Player(x=pBx, y=pBy, has_ball=pBball, p_id='B')
#
#             world = World()
#             world.set_world_size(x=self.cols, y=self.rows)
#             world.place_player(player_a, player_id='A')
#             world.place_player(player_b, player_id='B')
#             world.set_goals(100, 0, 'A')
#             world.set_goals(100, 3, 'B')
#
#             actions = {'A': actions_A, 'B': actions_B}
#             new_state, rewards, goal = world.move(actions)
#
#             rA = rewards['A']
#
#             Value = 0.0
#
#             for action_ in self.total_actions:
#                 Value += self.QvaluesA[new_state][action_] * self.piA[new_state][action_]
#             QV += self.piB[s][actions_B] * (rA + self.gamma * Value)
#
#         self.QValue_backupA[s][actions_A] = (0.1 * self.QvaluesA[s][actions_A]) + (0.9 * QV)
#
#
# #####################################################################
# #MDP
#     for s in range(self.numberOfStates):
#
#             # Terminal - Set Q as Reward (no actions in terminal states)
#             # - alternatively, one action which is 'exit' which gets full reward
#             if type(self.gridWorld.grid[s]) != str:
#                 for a in range(self.numberOfActions):
#                     self.Q_bu[s][a] = self.gridWorld.getReward(s)
#                 continue
#
#             # For walls , possibly (to conform to other setups)
#             # if grid[s] == '#':
#             #     continue
#
#             # Loop through all actions in current state s
#             for a in range(self.numberOfActions):
#
#
#                 succs = self.gridWorld.getNextStatesAndProbs(s,a)
#
#
#                 Value = 0.0
#                 for _, s_prime, prob in succs:
#                     tempValue = 0.0
#                     for a_prime in range(self.numberOfActions):
#                         tempValue += self.Q[s_prime][a_prime] * self.pi[s_prime][a_prime]
#                     Value += prob * (self.gridWorld.getReward(s) + self.gamma * tempValue)
#
#                 self.Q_bu[s][a] = Value
#
#
# ###############################################################
# # General LONR
#
#     for n in range(N): #for every players
#
#         for s in range(S): #For every state
#
#             for action_current in range(A(n)): # for actions in player n
#
#                 # Get successor states
#                 # For 1 player mdp, other player = determism/non-determinism
#                 succs = getProbsAndNextStates(s, p_n, p_not_n)
#
#                 Value = 0.0
#                 # For non-determ mdp, prob is stochasticity
#                 # For 2 player MG, prob is other agent
#                 for _, s_prime, prob in succs:
#                     tempValue = 0.0
#                     for a_prime in range(self.numberOfActions):
#                         tempValue += self.Q[s_prime][a_prime] * self.pi[s_prime][a_prime]
#                     Value += prob * (self.gridWorld.getReward(s) + self.gamma * tempValue)
#
#                 self.QValue_backupA[s][actions_A] = (0.1 * self.QvaluesA[s][actions_A]) + (0.9 * QV)


###############################################################################################

Double Q

 def DoubleQUpdate(self, n, s, a_current2, randomS=None):

        # print("IN:  s: ", s, " RANDOMS: ", randomS)
        qID = np.random.randint(0,2)
        for a_current in a_current2:

            if self.M.isTerminal(s):

                # Value iteration
                reward = self.M.getReward(s, a_current, 0, 0)
                # if reward == None:
                #     reward = 0.0
                if qID == 0:
                    self.M.QA_bu[n][s][a_current] = reward
                else:
                    self.M.QB_bu[n][s][a_current] = reward
                # self.M.Q_bu[n][s][a_current] = reward
                continue


            # Get next states and transition probs for MDP
            # Get next states and other players policy for MG
            succs = self.M.getNextStatesAndProbs(s, a_current, n)
            # print("s: ", s, " a:", a_current, "   succs: ", succs)

            # print("  SUCCS: ", succs)
            Value = 0.0

            # Loop thru each next state and prob
            for s_prime, prob, reward in succs:
                # print(" - s_prime: ", s_prime, "  prob: ", prob, "  reward: ", reward)
                tempValue = 0.0

                # Loop thru actions in s_prime for player n
                for a_current_prime in self.M.getActions(s_prime, n):
                    if qID == 0:
                        tempValue += self.M.QB[n][self.M.getStateRep(s_prime)][a_current_prime] * self.M.pi[n][self.M.getStateRep(s_prime)][a_current_prime]
                    else:
                        tempValue += self.M.QA[n][self.M.getStateRep(s_prime)][a_current_prime] * self.M.pi[n][self.M.getStateRep(s_prime)][a_current_prime]

                reww = self.M.getReward(s, a_current, n, 0)

                if reww is None:
                    Value += prob * (reward + self.gamma * tempValue)
                else:
                    Value += prob * (reww + self.gamma * tempValue)

            if qID == 0:
                self.M.QA_bu[n][self.M.getStateRep(s)][a_current] = (1.0 - self.alpha) * self.M.QA[n][self.M.getStateRep(s)][a_current] + (self.alpha) * Value
            else:
                self.M.QB_bu[n][self.M.getStateRep(s)][a_current] = (1.0 - self.alpha) * self.M.QB[n][self.M.getStateRep(s)][a_current] + (self.alpha) * Value
            self.M.QASums[n][self.M.getStateRep(s)][a_current] += self.M.QA[n][self.M.getStateRep(s)][a_current]
            self.M.QBSums[n][self.M.getStateRep(s)][a_current] += self.M.QB[n][self.M.getStateRep(s)][a_current]
            self.M.QATouched[n][self.M.getStateRep(s)][a_current] += 1.0
            self.M.QBTouched[n][self.M.getStateRep(s)][a_current] += 1.0

    def DoubleQBackup(self, n, WW=None):

        # def QBackup(self, n, WW=None):
        #
        for s in self.M.getStates():
            for a in self.M.getActions(s, n):
                #print("QQ: ",  self.M.Q[n][s][a], " QQBU: ", self.M.Q_bu[n][s][a])
                self.M.QA[n][s][a] = self.M.QA_bu[n][s][a]
                self.M.QB[n][s][a] = self.M.QB_bu[n][s][a]

                # self.M.QASums[n][self.M.getStateRep(s)][a] += self.M.QA[n][self.M.getStateRep(s)][a]
                # self.M.QBSums[n][self.M.getStateRep(s)][a] += self.M.QB[n][self.M.getStateRep(s)][a]


    def DoubleQregretUpdate(self, n, currentState, t):

        qID = np.random.randint(0, 2)
        iters = t + 1
        alphaR = 3.0 / 2.0  # accum pos regrets
        betaR = 0.0  # accum neg regrets
        gammaR = 2.0  # contribution to avg strategy

        alphaW = pow(iters, self.alphaDCFR)
        alphaWeight = (alphaW / (alphaW + 1))

        betaW = pow(iters, self.betaDCFR)
        betaWeight = (betaW / (betaW + 1))

        gammaWeight = pow((iters / (iters + 1)), self.gammaDCFR)

        # Calculate regret - this variable name needs a better name
        target = 0.0

        # Skip terminal states
        if self.M.isTerminal(self.M.getStateRep(currentState)) == True:
            return

        for a in self.M.getActions(currentState, n):
            if qID == 0:
                target += self.M.QA[n][self.M.getStateRep(currentState)][a] * self.M.pi[n][self.M.getStateRep(currentState)][a]
            else:
                target += self.M.QB[n][self.M.getStateRep(currentState)][a] * self.M.pi[n][self.M.getStateRep(currentState)][a]

        for a in self.M.getActions(currentState, n):
            if qID == 0:
                action_regret = self.M.QA[n][self.M.getStateRep(currentState)][a] - target
            else:
                action_regret = self.M.QB[n][self.M.getStateRep(currentState)][a] - target

            if self.DCFR or self.RMPLUS:
                if action_regret > 0:
                    action_regret *= alphaWeight
                else:
                    action_regret *= betaWeight

            # RMPLUS = False
            if self.RMPLUS:
                self.M.regret_sums[n][self.M.getStateRep(currentState)][a] = max(0.0, self.M.regret_sums[n][self.M.getStateRep(currentState)][a] + action_regret)
            else:
                self.M.regret_sums[n][self.M.getStateRep(currentState)][a] += action_regret

        # Skip terminal states
        # if self.M.isTerminal(self.M.getStateRep(currentState)) == True:
        #     continue

        for a in self.M.getActions(self.M.getStateRep(currentState), n):

            if self.M.isTerminal(currentState):
                continue
            # Sum up total regret
            rgrt_sum = 0.0
            for k in self.M.regret_sums[n][self.M.getStateRep(currentState)].keys():
                rgrt_sum += self.M.regret_sums[n][self.M.getStateRep(currentState)][k] if self.M.regret_sums[n][self.M.getStateRep(currentState)][k] > 0 else 0.0
                # if np.random.randint(0, 100) < 5:
                #     print("REGRET SUM REMINDER")
            # Check if this is a "trick"
            # Check if this can go here or not
            if rgrt_sum > 0:
                self.M.pi[n][self.M.getStateRep(currentState)][a] = (max(self.M.regret_sums[n][self.M.getStateRep(currentState)][a], 0.)) / rgrt_sum
            else:
                #print("REG SUMS: ", rgrt_sum)
                self.M.pi[n][self.M.getStateRep(currentState)][a] = 1.0 / len(self.M.getActions(self.M.getStateRep(currentState), n))
                #print("  PI: ", self.M.pi[n][self.M.getStateRep(currentState)][a])

            # Add to policy sum
            if self.DCFR or self.RMPLUS:
                self.M.pi_sums[n][self.M.getStateRep(currentState)][a] += self.M.pi[n][self.M.getStateRep(currentState)][a] * gammaWeight
            else:

                self.M.pi_sums[n][self.M.getStateRep(currentState)][a] += self.M.pi[n][self.M.getStateRep(currentState)][a]


################################################################################################

#########################################################################
# Double Q LONR Learning
#########################################################################

# print("Begin Double Q tests on GridWorld")
#
# print("Creating Grid World")
# gridMDP = Grid(noise=0.0)
#
# parameters = {'alpha': 1.0, 'epsilon': None, 'gamma': 1.0}
# regret_minimizers = {'RM': True, 'RMPlus': False, 'DCFR': False} # DCFR parameters alpha, beta, gamma?
#
# lonrAgent = LONR_DOUBLEQ(M=gridMDP, parameters=parameters, regret_minimizers=regret_minimizers, dcfr={})
#
# # Train via VI
# iters=1000
# lonrAgent.lonr_train(iterations=iters, log=1250)
# print("[North, East, South, West]")
# print("Note: these are Q, not QAvg")
# print("Q for: Bottom left (start state)")
# print(gridMDP.QA[0][36])
#
# print("Q for: State above bottom right terminal")
# print(gridMDP.QA[0][35])
# print("End GridWorld VI with determism")
#
# print("QA Avg")
# for k in sorted(gridMDP.QASums[0].keys()):
#     tot = 0.0
#     print(k, ": ", end='')
#     for kk in gridMDP.QASums[0][k].keys():
#         touched = max(gridMDP.QATouched[0][k][kk], 1.0)
#         #if touched == 0: touched = 1.0
#         print(kk, ": ", gridMDP.QASums[0][k][kk] / touched, " ", end='')
#     print("")
# print("")
# print("QB Avg")
# for k in sorted(gridMDP.QBSums[0].keys()):
#     tot = 0.0
#     print(k, ": ", end='')
#     for kk in gridMDP.QBSums[0][k].keys():
#         touched = max(gridMDP.QBTouched[0][k][kk], 1.0)
#         #if touched == 0: touched = 1.0
#         print(kk, ": ", gridMDP.QBSums[0][k][kk] / touched, " ", end='')
#     print("")
#
# print("QA")
# for k in sorted(gridMDP.QA[0].keys()):
#     tot = 0.0
#     print(k, ": ", end='')
#     for kk in gridMDP.QA[0][k].keys():
#         touched = 1.0#max(gridMDP.QBTouched[0][k][kk], 1.0)
#         #if touched == 0: touched = 1.0
#         print(kk, ": ", gridMDP.QA[0][k][kk] / touched, " ", end='')
#     print("")

# End Double Q

##############################################################################



#########################################################################
# SOCCER GAME
#########################################################################

# Create soccer game MG (inherits from MDP class)
# print("Begin Soccer VI")
# print("THIS CURRENT SETTING WORKS")
# soccer = SoccerGame()

# Create LONR agent, feed in soccer
# A25
# ** | ** | ** | ** | ** | **
# ** | gB |    | A+ | gA | **
# ** | gB | B  |    | gA | **
# ** | ** | ** | ** | ** | **

# FIX TODAY
# lonrAgent = LONR(M=soccer, alpha=0.99, gamma=0.99, alphaDecay=1.0, RMPLUS=False, DCFR=True, VI=True)
#
# # Train via VI (4500 to get PI to converge)
# lonrAgent.lonr_value_iteration(iterations=4500, log=50)
#
# # Test of the learned policy:
#
# # Normalize pi sums here for now
# # soccer.normalize_pisums()
# # print("A21pisums Player A", soccer.pi_sums[0]["A21"])
# # print("A21Q  Player A", soccer.Q[0]["A21"])
# # print("")
# # print("A21pisums Player B", soccer.pi_sums[1]["A21"])
# # print("A21Q  Player B", soccer.Q[1]["A21"])
#
# print("")
# print("A25pisums Player A", soccer.pi_sums[0]["A25"])
# print("A25Q  Player A", soccer.Q[0]["A25"])
# print("")
# print("A25pisums Player B", soccer.pi_sums[1]["A25"])
# print("A25Q  Player B", soccer.Q[1]["A25"])


#
# # This plays face-to-face start state, player A has the ball
# # This is 66/33 win/lose for playerA. Should double check that is correct
# #
# print("Playing 50,000 games where:")
# print(" - Players are facing each other")
# print(" - Player A always starts with ball")
# soccer.play(iterations=50000, log=10000)
#
# print("")
#
# # Play random games, mix who has ball at start, positions, etc
# print("Playing 50,000 games where: all initial conditions are randomized")
# soccer.play_random(iterations=50000,log=10000)
#
# print("")
#
# print("Print out of game states of interest, here is board for reminder")
#
# print(" 0  1  2  3")
# print(" 4  5  6  7")
# print("")
# print("Note: [North, South, East, West]")
# print("B has ball in 6, A in 2")
# print("PiB26 A: ", soccer.pi[0]["B26"])
# print("PiB26 B: ", soccer.pi[1]["B26"])
# print("")
# print("B has ball in 5, A in 2")
# print("PiB25 A: ", soccer.pi[0]["B25"])
# print("PiB25 B: ", soccer.pi[1]["B25"])
#
# print("")
# print("A has ball in 2, B in 5")
# print("PiA25 A: ", soccer.pi[0]["A25"])
# print("PiA25 B: ", soccer.pi[1]["A25"])
# print("")
# print("A has ball in 1, B in 5")
# print("PiA15 A: ", soccer.pi[0]["A15"])
# print("PiA15 B: ", soccer.pi[1]["A15"])
# print("")
# print("End Soccer VI")
# print("")


####################################################################################

class LONR_DOUBLEQ(LONR):
    def __init__(self, M=None, parameters=None, regret_minimizers=None, dcfr=None, randomize=True):
        super().__init__(M=M, parameters=parameters, regret_minimizers=regret_minimizers, dcfr=dcfr)

    ################################################################
    # LONR Value Iteration
    ################################################################
    def lonr_train(self, iterations=-1, log=-1, randomize=False):

        if log != -1: print("Starting training..")

        for t in range(1, iterations + 1):

            if (t + 0) % log == 0:
                print("Iteration: ", t + 0, " alpha: ", self.alpha, " gamma: ", self.gamma)

            # Call one full update via LONR-V
            self._lonr_train(t=t)

            # No-op unless alphaDecay is not 1.0
            self.alpha *= self.alphaDecay
            self.alpha = max(0.0, self.alpha)

            if randomize:
                if self.M.version == 1:
                    self.M.version = 2
                else:
                    self.M.version = 1

        if log != -1: print("Finish Training")

    ######################################
    ## Double q train
    ######################################
    def _lonr_train(self, t):
        """ One full update via LONR-V
        """

        # Q Update

        for n in range(self.M.N):

            # Loop through all states
            for s in self.M.getStates():

                if self.M.isTerminal(s):
                    a = self.M.getActions(s, 0)
                    self.DoubleQUpdate(n, s, a, randomS=s)

                    continue

                # Loop through actions of current player n
                # for a in self.M.getActions(s, n):
                a = self.M.getActions(self.M.getStateRep(s), n)
                # if self.randomize == False:
                self.DoubleQUpdate(n, s, a, randomS=None)
                # else:
                #    self.QUpdate(n, s=s, a_current2=a, randomS=s)

        # Q Backup
        for n in range(self.M.N):
            self.DoubleQBackup(n)

        for n in range(self.M.N):
            for s in self.M.getStates():
                self.DoubleQregretUpdate(n, s, t)


##################################################################################


####################################################################
# LONR Asynchronous Value Iteration - Version 2&3
####################################################################
class LONR_A2(LONR):

    def __init__(self, M=None, parameters=None, regret_minimizers=None):
        super().__init__(M=M, parameters=parameters, regret_minimizers=regret_minimizers)

    def lonr_train(self, iterations=-1, log=-1, randomize=False):


        if log != -1: print("Starting training..")
        for t in range(1, iterations+1):

            if t % log == 0:
                print("Iteration: ", t, " alpha:", self.alpha, " epsilon: ", self.epsilon)

            self._lonr_train(t=t)#, totalIterations=iterations, randomized=randomized)

            self.alpha *= self.alphaDecay
            self.alpha = max(0.0, self.alpha)

            if randomize:
                if self.M.version == 1:
                    self.M.version = 2
                    # self.M.startState = "rootTR"
                else:
                    self.M.version = 1
                    # self.M.startState = "rootTL"


        if log != -1: print("Finished Training")

    ################################################################
    # LONR Asynchronous Value Iteration 2&3
    ################################################################
    def _lonr_train(self, t):


        currentState = self.M.startState


        done = False
        n = 0  # One player

        # Episode loop - until terminal state is reached
        while done == False:

            #print("CS: ", currentState)
            # if self.M.isTerminal(self.M.getStateRep(currentState)) == True:
            #     done = True
            #     continue

            # 1. Pick a random action epsilon greedy
            if np.random.randint(0, 100) < int(self.epsilon):
                totalActions = self.M.getActions(currentState, 0)
                randomAction = np.random.randint(0, len(totalActions))
                randomAction = totalActions[randomAction]

            else:

                totalActions = []
                totalActionsProbs = []
                ta = self.M.getActions(currentState, 0)
                for action in ta:
                    totalActions.append(action)
                    totalActionsProbs.append(self.M.pi[n][self.M.getStateRep(currentState)][action])

                randomAction = np.random.choice(totalActions, p=totalActionsProbs)

            #print("  randomAction: ", randomAction)
            # 2. Get the possible successor states
            nextPossStates = self.M.getNextStatesAndProbsGrid(currentState, randomAction, 0)

            finalAction = None
            # Pick a successor state as nextState
            # If there is only one successor state, pick that
            if len(nextPossStates) == 0:
                lalala = 1
                nextState = None
                nextProb  = 1.0
                nextReward = 0.0
                nextAction = None
            elif len(nextPossStates) == 1:
                # nextPossStates is list of lists
                # nextPossStates = [[next_state, prob, reward]]
                nextState = nextPossStates[0][0]
                nextProb = nextPossStates[0][1]
                nextReward = nextPossStates[0][2]
                nextAction = nextPossStates[0][3]
                #nextReward = self.M.getReward(currentState, nextAction, 0, 0)
            else:
                nextStates = []
                nextStateProbs = []
                nextStateRewards = []
                nextStateActions = []
                for ns, nsp, nsw, na in nextPossStates:
                    nextStates.append(ns)
                    nextStateProbs.append(nsp)
                    nextStateRewards.append(nsw)
                    nextStateActions.append(na)

                #print("NS: ", nextStates)
                nextIndex = np.random.choice(len(nextStates), p=nextStateProbs)
                #print("NI: ", nextIndex)
                nextState = nextStates[nextIndex]#np.random.choice(nextStates, p=nextStateProbs)
                nextProb = nextStateProbs[nextIndex]
                nextReward = nextStateRewards[nextIndex]
                nextAction = nextStateActions[nextIndex]

                nextReward = self.M.getReward(currentState, nextAction,0,0)


            tempValue = 0.0

            # Loop thru actions in s_prime for player n
            if nextState != None:
                for a_current_prime in self.M.getActions(nextState, n):
                    tempValue += self.M.Q[n][self.M.getStateRep(nextState)][a_current_prime] * self.M.pi[n][self.M.getStateRep(nextState)][a_current_prime]

            Value = nextProb * (nextReward + self.gamma * tempValue)

            if self.M.getStateRep(currentState) != None:
                self.M.Q_bu[n][self.M.getStateRep(currentState)][randomAction] = (1.0 - self.alpha)*self.M.Q[n][self.M.getStateRep(currentState)][randomAction] + (self.alpha)*Value

            # Check this - tiger game stuff
            if self.M.getStateRep(currentState) == None:
                done = True
                continue

            # Check this - catching terminals
            if self.M.isTerminal(self.M.getStateRep(currentState)):
                a = self.M.getActions(currentState, 0)
                self.QUpdate(n, self.M.getStateRep(currentState), a, self.M.getStateRep(currentState))
                for n in range(self.M.N):
                    self.QBackup(n)
                done = True
                continue

            self.M.Q[0][self.M.getStateRep(currentState)][randomAction] = self.M.Q_bu[n][self.M.getStateRep(currentState)][randomAction]
            self.M.QSums[0][self.M.getStateRep(currentState)][randomAction] += self.M.Q[n][self.M.getStateRep(currentState)][randomAction]
            self.M.QTouched[n][self.M.getStateRep(currentState)][randomAction] += 1.0



            # if self.M.isTerminal(self.M.getStateRep(currentState)):
            #     a = self.M.getActions(self.M.getStateRep(currentState), 0)
            #     self.QUpdate(n, self.M.getStateRep(currentState), a, self.M.getStateRep(currentState))
            #     done = True
            #     continue


            # Get possible actions
            #a = self.M.getActions(currentState, 0)

            # Update Q of actions
            #self.QUpdate(n, currentState, a, currentState)


            # Q Backup
            # for n in range(self.M.N):
            #     self.QBackup(n)


            # Don't update terminal states
            # Check if these are ever even hit
            if self.M.isTerminal(self.M.getStateRep(currentState)) == True:
                done = True
                continue

            if self.M.getStateRep(currentState) == None:
                done = True
                continue

            if nextState == None:
                done = True
            # Update regrets
            self.regretUpdate(n, self.M.getStateRep(currentState), t)


            currentState = nextState





class LONR_A3(LONR):

    def __init__(self, M=None, parameters=None, regret_minimizers=None):
        super().__init__(M=M, parameters=parameters, regret_minimizers=regret_minimizers)

    def lonr_train(self, iterations=-1, log=-1, randomize=False):


        if log != -1: print("Starting training..")
        for t in range(1, iterations+1):

            if (t+1) % log == 0:
                print("Iteration: ", t+1, " alpha:", self.alpha, " epsilon: ", self.epsilon)

            self._lonr_train(t=t)#, totalIterations=iterations, randomized=randomized)

            self.alpha *= self.alphaDecay
            self.alpha = max(0.0, self.alpha)

            if randomize:

                TigerOnLeftProb = self.M.TLProb
                v = np.random.choice([1,2], p=[TigerOnLeftProb, 1.0-TigerOnLeftProb])
                self.M.version = v
                # if self.M.version == 1:
                #     self.M.version = 2
                #     # self.M.startState = "rootTR"
                # else:
                #     self.M.version = 1
                #     # self.M.startState = "rootTL"


        if log != -1: print("Finished Training")

    ################################################################
    # LONR Asynchronous Value Iteration
    ################################################################
    def _lonr_train(self, t):


        currentState = self.M.startState


        done = False
        n = 0  # One player

        # Episode loop - until terminal state is reached
        while done == False:

            # Check this - tiger game stuff
            if self.M.getStateRep(currentState) == None:
                done = True
                continue

            # Check this - catching terminals
            if self.M.isTerminal(self.M.getStateRep(currentState)):
                a = self.M.getActions(currentState, 0)
                self.QUpdate(n, self.M.getStateRep(currentState), a, self.M.getStateRep(currentState))
                for n in range(self.M.N):
                    self.QBackup(n)
                done = True
                continue

            # if self.M.isTerminal(self.M.getStateRep(currentState)):
            #     a = self.M.getActions(self.M.getStateRep(currentState), 0)
            #     self.QUpdate(n, self.M.getStateRep(currentState), a, self.M.getStateRep(currentState))
            #     done = True
            #     continue


            # Get possible actions
            a = self.M.getActions(currentState, 0)

            # Update Q of actions
            self.QUpdate(n, currentState, a, currentState)


            # Q Backup
            for n in range(self.M.N):
                self.QBackup(n)

            # Don't update terminal states
            # Check if these are ever even hit
            if self.M.isTerminal(self.M.getStateRep(currentState)) == True:
                done = True
                continue

            if self.M.getStateRep(currentState) == None:
                done = True
                continue

            # Update regrets
            self.regretUpdate(n, self.M.getStateRep(currentState), t)

            # Epsilon Greedy action selection
            if np.random.randint(0, 100) < int(self.epsilon):
                totalActions = self.M.getActions(currentState, 0)
                randomAction = np.random.randint(0, len(totalActions))
                randomAction = totalActions[randomAction]

            else:

                totalActions = []
                totalActionsProbs = []
                ta = self.M.getActions(currentState, 0)
                for action in ta:
                    totalActions.append(action)
                    totalActionsProbs.append(self.M.pi[n][self.M.getStateRep(currentState)][action])

                randomAction = np.random.choice(totalActions, p=totalActionsProbs)



            nextPossStates = self.M.getNextStatesAndProbs(currentState, randomAction, 0)

            # If there is only one successor state, pick that
            if len(nextPossStates) == 1:
                # nextPossStates is list of lists
                # nextPossStates = [[next_state, prob, reward]]
                currentState = nextPossStates[0][0]
            else:
                nextStates = []
                nextStateProbs = []
                for ns, nsp, _ in nextPossStates:
                    nextStates.append(ns)
                    nextStateProbs.append(nsp)

                currentState = np.random.choice(nextStates, p=nextStateProbs)

    ################################################################
    # QLearning
    ################################################################


    def getQValue(self, state, action):
        return self.M.Q[0][state][action]

    def computeValueFromQValues(self, state):
        legalActions = self.M.getActions(state,0)
        if len(legalActions) != 0:
            qValues = []
            for action in legalActions:
                qValues.append(self.getQValue(state, action))
            return max(qValues)
        else:

            return 0.0
        #util.raiseNotDefined()

    def computeActionFromQValues(self, state):

        legalActions = self.M.getActions(state, 0)

        if len(legalActions) != 0:
            stateValues = []
            possibleActions = []
            for action in legalActions:
                stateValues.append(self.getQValue(state, action))
                possibleActions.append(action)

            maxValue = max(stateValues)
            bestIndices = [index for index in range(len(stateValues)) if stateValues[index] == maxValue]
            chosenIndex = np.random.choice(bestIndices)
            return possibleActions[chosenIndex]
        else:

            return "exit"

    def getAction(self, state):
        if self.M.isTerminal(state):
            return "exit"
        if np.random.randint(0, 100) < self.epsilon:
            return np.random.randint(0, 4)
        else:
            action = self.getPolicy(state)
            return action

    def update(self, state, action, nextState, reward):
        gamma = self.gamma
        alpha = self.alpha

        sample = reward + gamma * self.getValue(nextState)
        self.M.Q[0][state][action] = self.M.Q[0][state][action] + self.alpha * (sample - self.M.Q[0][state][action])

    def getPolicy(self, state):
        return self.computeActionFromQValues(state)

    def getValue(self, state):
        return self.computeValueFromQValues(state)


    def qlearning_train(self, iterations=-1, log=-1, randomize=False):

        if log != -1: print("Starting training..")

        for t in range(1, iterations + 1):

            if (t + 0) % log == 0:
                print("Iteration: ", t + 0, " alpha: ", self.alpha, " gamma: ", self.gamma, " alphaDecay: ", self.alphaDecay)

            # Call one full update via LONR-V
            self._qlearning_train(t=t)

            # No-op unless alphaDecay is not 1.0
            self.alpha *= self.alphaDecay
            self.alpha = max(0.0, self.alpha)

            # if randomize:
            #     if self.M.version == 1:
            #         self.M.version = 2
            #     else:
            #         self.M.version = 1

        if log != -1: print("Finish Training")


    def _qlearning_train(self, t):


        currentState = self.M.startState

        done = False
        n = 0  # One player

        # Episode loop - until terminal state is reached
        while done == False:

            # print("Current State: ", currentState)

            if self.M.isTerminal(currentState):
                a = self.M.getActions(currentState, 0)
                for a in self.M.getActions(currentState, 0):
                    self.M.Q[0][currentState][a] = self.M.getReward(currentState, a, 0, 0)#(1.0-self.alpha)*self.M.Q[0][currentState][a] + self.alpha * self.M.getReward(currentState, a, 0, 0)
                done = True
                continue


            randomAction = self.getAction(currentState)

            s_prime = self.M.getMove(currentState, randomAction)
            rew = self.M.livingReward# -1.0# succs[0][2]

            self.update(currentState, randomAction, s_prime, rew)

            currentState = s_prime

#################################################################################









################################################################
# MISC
#
#     def _lonr_value_iteration(self, t):
#
#         WW = None
#         # Q Update
#         # All except Tiger Game
#         # if self.randomize == False:
#
#         for n in range(self.M.N):
#
#             # Loop through all states
#             for s in self.M.getStates():
#
#                 # Loop through actions of current player n
#                 # for a in self.M.getActions(s, n):
#                 a = self.M.getActions(self.M.getStateRep(s), n)
#                 if self.randomize == False:
#                     self.QUpdate(n, s, a, randomS=None)
#                 else:
#                     self.QUpdate(n, s=s, a_current2=a, randomS=s)
#
#         # Tiger Game
#         # elif self.randomize == True:
#         #     print("Tiger game - value iteration")
#         #     r = np.random.randint(0, 2)
#         #     #if r == 0:
#         #     if self.alt == 1:
#         #         WW = self.M.totalStatesLeft
#         #         self.alt = 2
#         #     else:
#         #         WW = self.M.totalStatesRight
#         #         self.alt = 1
#         #
#         #     print("WW: ")
#         #     print(WW)
#         #     for n in range(self.M.N):
#         #
#         #         # Loop through all states
#         #         for s in WW: #self.M.getStates(): #WW:
#         #
#         #             print(" S in loop: ", s)
#         #             # Loop through actions of current player n
#         #             # for a in self.M.getActions(s, n):
#         #             aa = self.M.getActions(s, n)
#         #             # if self.randomize == False:
#         #             #     self.QUpdate(n, s, a, randomS=None)
#         #             # else:
#         #             print("AA: ", aa)
#         #             for a in aa:
#         #                 #print(" a: ", a)
#         #                 al = []
#         #                 al.append(a)
#         #                 print(" a: ", a)
#         #                 self.QUpdate(n, s=s, a_current2=al, randomS=s)
#
#         # Q Backup
#         for n in range(self.M.N):
#             self.QBackup(n) #, WW=WW)
#
#         # Update regret sums, pi, pi sums
#
#         if self.randomize == False:
#             for n in range(self.M.N):
#                 for s in self.M.getStates():
#                     self.regretUpdate(n, s, t)
#         else:
#             for n in range(self.M.N):
#                 for s in self.M.getStates(): #WW: #self.M.totalStatesLeft: #WW:
#                     if self.M.isTerminal(s): continue
#                     self.regretUpdate(n, s, t)
#
#
#
#
#
#     def lonr_online(self, iterations=-1, log=-1, randomized=False):
#
#         print("Starting training..")
#         for t in range(1, iterations+1):
#
#             if (t+1) % log == 0:
#                 print("Iteration: ", t+1, " alpha:", self.alpha, " epsilon: ", self.epsilon)
#
#             self._lonr_online(t=t, totalIterations=iterations, randomized=randomized)
#
#             self.alpha *= self.alphaDecay
#             self.alpha = max(0.0, self.alpha)
#
#
#         print("Finished Training")
#
#
#     def _lonr_online(self, t=0, totalIterations=-1, randomized=False):
#         """
#
#         :param t: current iteration
#         :param totalIterations: total iterations
#         :param randomized: Randomizes the start state.
#                         True: For Tiger Game, to switch between TigerOnLeft, TigerOnRight
#                         False: For GridWorld, only one start state.
#         :return:
#         """
#
#
#         # For Tiger Game, randomize the MDP it sees
#         if randomized:
#
#             # Get the top root
#             startStates = self.M.getNextStatesAndProbs(self.M.startState, None, 0)
#
#             totalStartStates = []
#             totalStartStateProbs = []
#
#             # Pick Tiger on Left/Right based on probability set in M
#             for nextState, nextStateProb, _ in startStates:
#                 totalStartStates.append(nextState)
#                 totalStartStateProbs.append(nextStateProb)
#
#             # Randomly pick TigerOnLeft/Right based on M.TLProb (1-M.TLProb = TRProb)
#             currentState = np.random.choice(totalStartStates, p=totalStartStateProbs)
#
#         # For GridWorld, set currentState to the startState
#         else:
#
#             currentState = self.M.startState
#
#         #print("RANDOMIZED: ", randomized)
#         done = False
#         n = 0  # One player
#
#
#
#         # Episode loop - until terminal state is reached
#         while done == False:
#
#             #print("Current state: ", currentState)
#             if self.EXP3 == False:
#                 if self.M.isTerminal(currentState) == True:
#                     # for a in self.M.getActions(currentState, 0):
#                     #     # Note, the reward is via the actual state, so there is no getStateRep()
#                     #     self.M.Q_bu[n][self.M.getStateRep(currentState)][a] = (1.0 - self.alpha) * self.M.Q[n][self.M.getStateRep(currentState)][a] + self.alpha * self.M.getReward(currentState,a,a,a)
#                     #print("TEEEEEEEEEEEEEEEETMMR")
#                     a = self.M.getActions(currentState, 0)
#                     if self.LONR == True:
#                         self.QUpdate(n, currentState, a, currentState)
#                     else:
#                         #print("LALALALLALA")
#                         self.QLearningUpdate(n, currentState, a, currentState)
#                     # print("DONE = ", done)
#                     done = True
#                     continue
#
#                 a = self.M.getActions(currentState, 0)
#                 if self.LONR == True and self.EXP3 == False:
#                     self.QUpdate(n, currentState, a, currentState)
#                 else:
#                     self.QLearningUpdate(n, currentState, a, currentState)
#                 # totStates keeps track of which states need Qvalue copying
#                 #   as not all states need to be backed up, only the ones visited
#                 # totStates = []
#                 # totStates.append(currentState)
#
#             # Q Backup
#             for n in range(self.M.N):
#                 self.QBackup(n)
#
#
#             # Don't update terminal states
#             if self.EXP3 == False:
#                 if self.M.isTerminal(self.M.getStateRep(currentState)) == True:
#                     done = True
#                     continue
#
#             if self.EXP3 == False:
#
#                 if self.LONR == True:
#                     self.regretUpdate(n, self.M.getStateRep(currentState), t)
#                 else:
#                     noRegretUpdate = True
#                     #self.QLearningPIUpdate()
#
#                 # Epsilon Greedy action selection
#                 if np.random.randint(0, 100) < int(self.epsilon):
#                     totalActions = self.M.getActions(currentState, 0)
#                     randomAction = np.random.randint(0, len(totalActions))
#                     randomAction = totalActions[randomAction]
#                     #print("RAndom action: ", randomAction)
#
#                 else:
#
#                     totalActions = []
#                     totalActionsProbs = []
#                     ta = self.M.getActions(currentState, 0)
#                     for action in ta:
#                         totalActions.append(action)
#                         totalActionsProbs.append(self.M.pi[n][self.M.getStateRep(currentState)][action])
#                     #print(totalActionsProbs)
#                     if self.LONR == True:
#                         randomAction = np.random.choice(totalActions, p=totalActionsProbs)
#                     else:
#                         randomActionValue = max(totalActionsProbs)
#                         randomAction = totalActionsProbs.index(randomActionValue)
#                         #print("Random Action: ", randomAction)
#
#             #Exp 3 is true
#             else:
#
#                 randomAction = self.exp3Update(n, currentState, 4, 0, 0.5, t)
#             # randomAction picked, now simulate taking action
#             #       GridWorld: This will handle non-determinism, if there is non-determinism
#             #       TigerGame: This will either get the one next state OR
#             #                       if action is LISTEN, it will return next state based on
#             #                       observation accuracy aka (85/15) or (15/85), which is
#             #                       equivalent to hearing a growl left or growl right
#             nextPossStates = self.M.getNextStatesAndProbs(currentState, randomAction, 0)
#             #print("SS: ", nextPossStates)
#
#             #print("NPS: ", nextPossStates)
#             # If there is only one successor state, pick that
#             if len(nextPossStates) == 1:
#                 # nextPossStates is list of lists
#                 # nextPossStates = [[next_state, prob, reward]]
#                 currentState = nextPossStates[0][0]
#             else:
#                 nextStates = []
#                 nextStateProbs = []
#                 for ns, nsp,_ in nextPossStates:
#                     nextStates.append(ns)
#                     nextStateProbs.append(nsp)
#                 #print("Nextposs: ", nextPossStates)
#                 currentState = np.random.choice(nextStates, p=nextStateProbs)
#             print("CURRENT STATE: ", currentState, )
#
#             if self.EXP3 == True:
#                 for n in range(self.M.N):
#                     self.QBackup(n)
#                 if self.M.isTerminal(self.M.getStateRep(currentState)) == True:
#                     done = True
#                     continue
#             # for n in range(self.M.N):
#             #     self.QBackup(n)
#             # if self.LONR == False:
#             #     aa = []
#             #     aa.append(randomAction)
#             #     self.QLearningUpdate(n, currentState, aa, currentState)
#             # print("Current state: ", currentState)






##################################################################################


    def lonr_policy_eval(self, iterations=-1, log=-1, randomize=False):
        if log != -1: print("Starting eval..")

        for n in range(self.M.N):
            for s in self.M.getStates():
                for a in self.M.getActions(s, n):
                    self.M.Q[n][s][a] = 0.0
                    self.M.Q_bu[n][s][a] = 0.0
                    self.M.QSums[n][s][a] = 0.0
                    self.M.QTouched[n][s][a] = 0.0

        self.totalIterations = iterations

        for t in range(1, iterations + 1):

            if (t + 0) % log == 0:
                print("Iteration: ", t + 0, " alpha: ", self.alpha, " gamma: ", self.gamma)

            # Call one full update via LONR-V
            self._lonr_policy_eval(t=t)

            # print("Q    : ", self.M.Q)
            # print("QBU  : ", self.M.Q_bu)
            # print("QSums: ", self.M.QSums)

            # No-op unless alphaDecay is not 1.0
            # self.alpha *= self.alphaDecay
            # self.alpha = max(0.0, self.alpha)
            #
            # if randomize:
            #     if self.M.version == 1:
            #         self.M.version = 2
            #     else:
            #         self.M.version = 1

        if log != -1: print("Finish Training")


    def _lonr_policy_eval(self, t):


        # Set start state or randomize start state
        currentState = self.M.startState

        # Settings
        done = False
        n = 0  # One player
        nextAction = None

        # Episode loop - until terminal state is reached
        while done == False:

            #print("Current state: ", currentState, " nextAction: ", nextAction, " t: ", t)

            # Periodic output
            if t % 12500 == 0:
                verbose = True
            else:
                verbose = False

            # Force terminal to be correct value based on reward
            if self.M.isTerminal(currentState):
                for a in self.M.getActions(currentState, n):
                    self.M.Q_bu[n][currentState][a] = self.M.getReward(currentState, a, 0, 0)
                    self.M.Q[n][currentState][a] = self.M.Q_bu[n][currentState][a]
                    self.M.QSums[0][currentState][a] += self.M.Q[0][currentState][a]
                    self.M.QTouched[0][currentState][a] += 1.0
                done = True
                continue

            # everything done in exp3, just get the action it used
            # Note: return action and state for non-determinism (might be different here)
            probList = []
            for gg in self.M.getActions(currentState, n):
                probList.append(self.M.pi_sums[n][currentState][gg])


            if len(self.M.getActions(currentState, n)) > 1:
                nextAction = np.random.choice([0,1,2,3], p=probList)
                #nextAction = np.argmax(probList)
            else:
                nextAction = "exit"


            #nextAction = self.exp3(currentState, t)

            s_prime = self.M.getMove(currentState, nextAction)
            # print("Current state: ", currentState, " looking at ", s_prime)

            rew = self.M.getReward(currentState, nextAction, 0, 0)

            Value = 0.0
            for a_prime in self.M.getActions(s_prime, n):
                # print("  Qvalue: ", self.M.Q[n][s_prime][a_prime], " a_prime: ", a_prime)
                # print("  pisums: ", self.M.pi_sums[n][s_prime][a_prime], " a_prime: ", a_prime)
                Value += self.M.pi_sums[n][s_prime][a_prime] * self.M.Q[n][s_prime][a_prime]

            # print("     rew: ", rew)
            # print("     Val: ", Value)
            x = rew + self.gamma * Value


            self.M.Q_bu[0][currentState][nextAction] = self.alpha * x + (1.0 - self.alpha) * self.M.Q[0][currentState][nextAction]  # (rew + self.gamma * Value)(1.0 / self.M.pi[0][currentState][nextAction]) *

            # print("X  : ", x)
            # print("Q  : ", self.M.Q[0][currentState][nextAction])
            # print("QBU: ", self.M.Q_bu[0][currentState][nextAction])

            for aa in self.M.Q[0][currentState].keys():

                # if aa == nextAction:
                self.M.Q[0][currentState][aa] = self.M.Q_bu[0][currentState][aa]
                self.M.QSums[0][currentState][aa] += self.M.Q[0][currentState][aa]
                #print("QSUMS: ", self.M.QSums[0][currentState][aa] )
                self.M.QTouched[0][currentState][aa] += 1.0
                # else:
                #     ddd = 3
                #     # self.M.QSums[0][currentState][aa] += self.M.Q[0][currentState][aa]
                #     # self.M.QTouched[0][currentState][aa] += 1.0
                #     self.M.Q[0][currentState][aa] = 0.0
                #     self.M.Q_bu[0][currentState][aa] = 0.0

            if verbose: print("")
            if verbose: print("Current State: ", currentState, "  currentAction: ", nextAction, " t=", t)

            nextState = self.M.getMove(currentState, nextAction)
            currentState = nextState

########################################################################################################

class LONR_B2(LONR):

    def __init__(self, M=None, parameters=None, regret_minimizers=None):
        super().__init__(M=M, parameters=parameters, regret_minimizers=regret_minimizers)

        # 1. Set all weights to 1.0 (done in GridWorld MDP)


    ################################################################
    # LONR Bandit
    ################################################################
    def lonr_train(self, iterations=-1, log=-1, randomize=False):

        if log != -1: print("Starting training..")
        gamma = 0.2#1.0
        for t in range(1, iterations + 1):

            if (t + 0) % log == 0:
                print("Iteration: ", t + 0, " alpha: ", self.alpha, " gamma: ", self.gamma, " othergamma: ", gamma)

            # Call one full update via LONR-V
            self._lonr_train(t=t, gamma=gamma)
            #gamma *= 0.99999
            #gamma = max(1.0 - (float((1.0/2.0)*t) / float(iterations )), 0.0)
        if log != -1: print("Finish Training")

    ######################################
    ## LONR Bandit
    ######################################
    def _lonr_train(self, t, gamma=0.0):


        currentState = self.M.startState

        #currentState = np.random.randint(0, 12)
        done = False
        n = 0  # One player

        # print("")
        # print("")
        # print("---------------------------------")
        # print("New episode")

        nextAction = None
        # Episode loop - until terminal state is reached
        c = 0
        visited = []
        while done == False:

            if currentState not in visited:
                visited.append(currentState)
            if t % 3500 == 0:
                verbose = True
            else:
                verbose = False
            # verbose = True
            if verbose: print("")
            if verbose: print("Current State: ", currentState, "  currentAction: ", nextAction, " t=", t)

            if self.M.isTerminal(currentState):
                for a in self.M.getActions(currentState, n):
                    self.M.Q[n][currentState][a] = self.M.getReward(currentState, a,0,0)
                    self.M.Q_bu[n][currentState][a] = self.M.getReward(currentState, a, 0, 0)
                    #print("Final: set s: ", currentState, " to: ", self.M.getReward(currentState, a,0,0))
                done = True
                continue

            nextAction = self.exp3(currentState, t, gamma=gamma)

            s_prime = self.M.getMove(currentState, nextAction)

            rew = self.M.getReward(currentState, nextAction, 0, 0)

            Value = 0.0
            for a_prime in self.M.getActions(s_prime, n):
                Value += self.M.pi[n][s_prime][a_prime] * self.M.Q[n][s_prime][a_prime]

            x = rew + self.gamma * Value

            self.M.Q_bu[0][currentState][nextAction] = (self.alpha * x ) + ((1 - self.alpha) * self.M.Q[0][currentState][nextAction]) #(rew + self.gamma * Value)(1.0 / self.M.pi[0][currentState][nextAction]) *
            #self.M.Q[0][currentState][nextAction] = self.M.Q_bu[0][currentState][nextAction]

            #self.regretUpdate2(0, currentState, t)

            nextState = self.M.getMove(currentState, nextAction)
            currentState = nextState


        for s in visited:
            for a in self.M.getActions(s, 0):
                self.M.Q[0][s][a] = self.M.Q_bu[0][s][a]
                self.M.QSums[0][s][a] += self.M.Q[0][s][a]
                self.M.QTouched[0][s][a] += 1.0

        for s in visited:
            for a in self.M.getActions(s, 0):
                self.regretUpdate2(0, s, t, a)
        visited = []
            #done = False
            # if self.M.isTerminal(currentState):
            #     done = True
            #     continue
            #
            # if nextState == 1:
            #     for i in range(100):
            #         print("JUST HIT 7777777777777777************************************************")

            #def exp3Update(self, n, currentState, numActions, t, rewardMin=-100.0, rewardMax=200.0):
            # nextAction = self.exp3Update(n, currentState, 4, t)
            #
            # self.regretUpdate(n, self.M.getStateRep(currentState), t)
            #
            # for n in range(self.M.N):
            #     for s in range(len(self.M.getStates())):
            #         for a in range(len(self.M.getActions(s,n))):
            #             self.M.Q[n][s][a] = self.M.Q_bu[n][s][a]
            #
            # nextState = self.M.getMove(currentState, nextAction)
            #
            # currentState = nextState
            #
            # if self.M.isTerminal(currentState):
            #     done = True
            #     # for a in self.M.getActions(currentState, n):
            #     #     self.M.Q[n][currentState][a] = self.M.getReward(currentState,a,0,0)
            #     continue




    def exp3(self, currentState, t, gamma=0.0):

        regrets = {}
        regretSum = 0.0
        for r in sorted(self.M.regret_sums[0][currentState].keys()):
            # print("Regret for action: ", r)
            regrets[r] = self.M.regret_sums[0][currentState][r]
            regretSum += self.M.regret_sums[0][currentState][r]

        # print("RegretSum: ", regretSum)
        #gamma = 0.25

        for r in sorted(self.M.pi[0][currentState].keys()):
            if regretSum > 0:
                self.M.pi[0][currentState][r] = (1.0 - gamma)*(max(regrets[r], 0.0) / regretSum) + (gamma / len(regrets))
            else:
                self.M.pi[0][currentState][r] = 1.0 / len(regrets)

            if regretSum > 0:
                self.M.pi_sums[0][currentState][r] += max(regrets[r], 0.0) / regretSum



        pi = {}
        pSum = 0.0
        for r in sorted(self.M.pi[0][currentState].keys()):
            pi[r] = self.M.pi[0][currentState][r]
            pSum = 1.0#+= pi[r]

        # print("PI: ", pi, " PiSum: ", sum(pi))
        piNorm = [p / sum(pi) for p in pi]
        piNorm = []
        for p in sorted(pi.keys()):
            piNorm.append(pi[p] / pSum)


        rAct = np.random.choice([0,1,2,3], p=piNorm)

        ###
        # if t % 6200 == 0:
        #
        #     for s in self.M.getStates():
        #         pi = {}
        #         pSum = 0.0
        #         for r in sorted(self.M.regret_sums[0][s].keys()):
        #             pi[r] = self.M.regret_sums[0][s][r]
        #             pSum += pi[r]
        #
        #         # print("PI: ", pi, " PiSum: ", sum(pi))
        #         piNorm = [p / sum(pi) for p in pi]
        #         piNorm = []
        #         if pSum <= 0:
        #             pSum = 1.0
        #         for p in sorted(pi.keys()):
        #             self.M.pi[0][s][p] = pi[p] / pSum


        return rAct
        # pi = [0, 0, 0, 0]
        #
        # regretSum = sum(regrets)
        # for r in range(4):
        #     if regretSum > 0:
        #         pi[r] = (1.0 - gamma)*(max(regrets[r], 0.0) / regretSum) + (gamma / len(regrets))
        #     else:
        #         pi[r] = 1.0 / len(regrets)

        # if t % 50 == 0:
        #     verbose = True
        # else:
        #     verbose = False
        #
        # rewardMin = -100.0
        # rewardMax = 200.0
        #
        # numActions = 4.0
        # n = 0
        # gamma = 0.15
        #
        # # get current weights for 0, 1, 2 ,3
        # weights = []
        # if verbose: print("Weights: ")
        # for w in sorted(self.M.regret_sums[n][currentState].keys()):
        #     if verbose: print("   ", w, ": ", self.M.regret_sums[n][currentState][w])
        #     weights.append(self.M.regret_sums[n][currentState][w] * gamma / float(numActions))
        #
        # if verbose: print("Final weights list: ", weights) # init: 1.0, 1.0, 1.0, 1.0
        #
        # # Get probDist from weights
        # pd = self.distr(weights, gamma)
        #
        # if verbose: print("ProbDist: ", pd) # init: 0.25, 0.25, 0.25, 0.25
        #
        # # Dist without randomness (for final pi sum calc)
        # piSUMDist = self.distr(weights, gamma=0.0)
        #
        # # set pi as probDistbution
        # for a in sorted(self.M.pi[n][currentState].keys()):
        #     if verbose: print("Setting pi of s: ", currentState, " a: ", a, " : ", pd[a])
        #     self.M.pi[n][currentState][a] = pd[a]
        #     self.M.pi_sums[n][currentState][a] += piSUMDist[a]
        #
        # # Select a random action from pd
        # # if sum(pd) != 1:
        # #     pd = [0.25, 0.25, 0.25, 0.25]
        # randomAction = np.random.choice([0,1,2,3], p=pd)
        # if verbose: print("Selected random action: ", randomAction)
        #
        # #Observe reward for action randomAction
        # rew = self.M.getReward(currentState, randomAction, 0, 0)
        # s_prime = self.M.getMove(currentState, randomAction)
        # Value = 0.0
        # for a_prime in self.M.getActions(s_prime, n):
        #     Value += self.M.pi[n][s_prime][a_prime] * self.M.Q[n][s_prime][a_prime]
        #
        # # This is the entire "reward", taken from paper
        # x = rew + self.gamma*Value
        #
        # # Scale the reward
        # scaledReward = (x - rewardMin) / (rewardMax - rewardMin)
        #
        # # Get expected reward
        # estimatedReward = scaledReward / pd[randomAction]
        #
        # # Add to running sum of rewards
        # self.M.runningRewards[0][currentState][randomAction] += estimatedReward
        # runningEstimatedReward = self.M.runningRewards[0][currentState][randomAction]
        #
        # # Find min reward
        # # currentRunningRewards = []
        # # for r in self.M.runningRewards[0][currentState].keys():
        # #     currentRunningRewards.append(self.M.runningRewards[0][currentState][r])
        # #
        # # minRunningReward = min(currentRunningRewards)
        # #
        # #
        # # for r in self.M.runningRewards[0][currentState].keys():
        # #     self.M.runningRewards[0][currentState][r] = self.M.runningRewards[0][currentState][r] - minRunningReward
        # #
        # # runningEstimatedReward = runningEstimatedReward - minRunningReward
        #
        #
        # # Find min weight
        # # currentRunningWeights = []
        # # for r in self.M.weights[0][currentState].keys():
        # #     currentRunningWeights.append(self.M.weights[0][currentState][r])
        # #
        # # minWeight = min(currentRunningWeights)
        # # if minWeight > 1:
        # #     for r in self.M.weights[0][currentState].keys():
        # #         self.M.weights[0][currentState][r] = self.M.weights[0][currentState][r] + minWeight
        #
        # # TODO: Cap of gap betwen max1 and max2
        #
        # # Set Weight for t+1
        # numActions = 4.0
        # if verbose: print(runningEstimatedReward)
        #
        # # self.M.weights[n][currentState][randomAction] += math.exp(runningEstimatedReward * gamma / float(numActions))
        # self.M.regret_sums[n][currentState][randomAction] += runningEstimatedReward #* gamma / float(numActions)
        # # if self.M.weights[n][currentState][randomAction] > 1000000:
        # #     self.M.weights[n][currentState][randomAction] = 1000000
        # #ooof had this: math.exp(math.exp(runningEstimatedReward * gamma / float(numActions)))

        #return randomAction

################################################################################


 # if maxWeight - maxWeight2 > thresh:
        #
        #     lp = []
        #     for w in weights:
        #         if w > maxWeight - maxWeight2:
        #             cc = gamma / len(weights)
        #         else:
        #             cc = 0.0 #(1.0 - gamma) * (w / theSum) + (gamma / len(weights))
        #         lp.append(cc)
        # else:

# get the max weight
        # #print("Weights: ", weights)
        # maxWeight = max(weights)
        # #print("Max Weight: ", maxWeight)
        # weigths_copy = weights.copy()
        # weigths_copy.remove(maxWeight)
        # maxWeight2 = max(weigths_copy)
        # #print(maxWeight, "  ", maxWeight2)
        #
        # #if maxWeight - maxWeight2 > 100000000.0:
        # #    gamma = 1.0
        #
        # thresh = 100000.0 #maxWeight - maxWeight2


################################################################################

# if WW is None:
        #     for s in self.M.getStates():
        #         for a in self.M.getActions(s, n):
        #             #self.M.Q[n][s][a] = self.M.Q_bu[n][s][a]
        #             self.M.Q[n][self.M.getStateRep(s)][a] = self.M.Q_bu[n][self.M.getStateRep(s)][a]
        #             self.M.QSums[n][self.M.getStateRep(s)][a] += self.M.Q[n][self.M.getStateRep(s)][a]
        #
        # else:
        #     for s in WW:
        #         for a in self.M.getActions(s, n):
        #             self.M.Q[n][self.M.getStateRep(s)][a] = self.M.Q_bu[n][self.M.getStateRep(s)][a]
        #             self.M.QSums[n][self.M.getStateRep(s)][a] += self.M.Q[n][self.M.getStateRep(s)][a]


##################################################################################

# # Update regret sums, pi, pi sums
        # if self.randomize == False:
        #     for n in range(self.M.N):
        #         for s in self.M.getStates():
        #             self.regretUpdate(n, s, t)
        # else:
        #     for n in range(self.M.N):
        #         for s in self.M.getStates(): #WW: #self.M.totalStatesLeft: #WW:
        #             if self.M.isTerminal(s): continue
        #             self.regretUpdate(n, s, t)


###################################################################################

# # For Tiger Game, randomize the MDP it sees
        # if randomized:
        #
        #     # Get the top root
        #     startStates = self.M.getNextStatesAndProbs(self.M.startState, None, 0)
        #
        #     totalStartStates = []
        #     totalStartStateProbs = []
        #
        #     # Pick Tiger on Left/Right based on probability set in M
        #     for nextState, nextStateProb, _ in startStates:
        #         totalStartStates.append(nextState)
        #         totalStartStateProbs.append(nextStateProb)
        #
        #     # Randomly pick TigerOnLeft/Right based on M.TLProb (1-M.TLProb = TRProb)
        #     currentState = np.random.choice(totalStartStates, p=totalStartStateProbs)
        #
        # # For GridWorld, set currentState to the startState
        # else:

#####################################################################################

# def lonr_value_iteration(self, iterations=-1, log=-1):
    #
    #
    #
    #     print("Starting training..")
    #     tt = 0
    #     for t in range(1, iterations+1):
    #
    #         if (t+1) % log == 0:
    #             print("Iteration: ", t+1, " alpha: ", self.alpha)
    #
    #
    #         self._lonr_value_iteration(t=t)
    #
    #         # No-op unless alphaDecay is not 1.0
    #         self.alpha *= self.alphaDecay
    #         self.alpha = max(0.0, self.alpha)
    #
    #     print("Finish Training")

#######################################################################################

 # ################################################################
    # # Q Update
    # ################################################################
    # def QUpdate(self, n, s, a_current2, randomS=None):
    #
    #     for a_current in a_current2:
    #
    #         if self.M.isTerminal(s):
    #
    #             # Value iteration
    #             # if self.VI == True:
    #             #     # All except tiger game
    #             reward = 0.0
    #             if randomS is None:
    #                 reward = self.M.getReward(s, a_current, 0, 0)
    #             if reward == None:
    #                 reward = 0.0
    #             self.M.Q_bu[n][s][a_current] = reward
    #             continue
    #
    #
    #
    #         # Get next states and transition probs for MDP
    #         # Get next states and other players policy for MG
    #         succs = self.M.getNextStatesAndProbs(s, a_current, n)
    #         # print("s: ", s, " a:", a_current, "   succs: ", succs)
    #
    #         # print("  SUCCS: ", succs)
    #         Value = 0.0
    #
    #         # Loop thru each next state and prob
    #         for s_prime, prob, reward in succs:
    #             # print(" - s_prime: ", s_prime, "  prob: ", prob, "  reward: ", reward)
    #             tempValue = 0.0
    #
    #             # Loop thru actions in s_prime for player n
    #             for a_current_prime in self.M.getActions(s_prime, n):
    #                 tempValue += self.M.Q[n][self.M.getStateRep(s_prime)][a_current_prime] * self.M.pi[n][self.M.getStateRep(s_prime)][a_current_prime]
    #
    #             print("tempValue: ", tempValue)
    #             reww = self.M.getReward(s, a_current, n, 0)
    #
    #             if reww is None:
    #                 Value += prob * (reward + self.gamma * tempValue)
    #             else:
    #                 Value += prob * (reww + self.gamma * tempValue)
    #
    #         print("Value: ", Value)
    #         self.M.Q_bu[n][self.M.getStateRep(s)][a_current] = (1.0 - self.alpha) * self.M.Q[n][self.M.getStateRep(s)][a_current] + (self.alpha) * Value
    #
    #     # # actions can be a list of actions or a single action
    #     # for a in actions:
    #     #
    #     #     if self.M.isTerminal(s):
    #     #         reward = self.M.getReward(s, a, 0, 0)
    #     #         # print("S: ", s, " a: ", a, " terminal: ", s, " reward: ", reward)
    #     #         self.M.Q_bu[n][self.M.getStateRep(s)][a] = reward
    #     #         continue
    #     #
    #     #
    #     #     # Get next states and transition probs for MDP
    #     #     # Get next states and other players policy for MG
    #     #     succs = self.M.getNextStatesAndProbs(s, a, n)
    #     #
    #     #     #print("SUCCS: ", succs)
    #     #     Value = 0.0
    #     #
    #     #     #Loop thru each next state and prob
    #     #     for s_prime, prob, rew in succs:
    #     #
    #     #         # print("  sprime: ", s_prime)
    #     #         tempValue = 0.0
    #     #
    #     #         # Loop thru actions in s_prime for player n
    #     #         ###if s_prime != None:
    #     #         for a_current_prime in self.M.getActions(s_prime, n):
    #     #             tempValue += self.M.Q[n][self.M.getStateRep(s_prime)][a_current_prime] * self.M.pi[n][self.M.getStateRep(s_prime)][a_current_prime]
    #     #
    #     #
    #     #         # This part is here to correct for the different implementatiosn where in some, the
    #     #         # terminals are included in the successors, and others where they are but FIX THIS
    #     #         reward = self.M.getReward(s, a, n, 0)
    #     #
    #     #         # print("Reward for s: ", s, " a: ", a, "  : ", rew)#ard)
    #     #         if reward is None:
    #     #             Value += prob * (rew + self.gamma * tempValue)
    #     #         else:
    #     #         #print("S: ", self.M.getStateRep(s), " a: ", a, " sprime: ", s_prime, " reward: ", reward)
    #     #             Value += prob * (reward + self.gamma * tempValue)
    #     #
    #     #     # Left in, as alpha=1.0 knocks off the first term.
    #     #     # print("Setting rep(s): ", self.M.getStateRep(s), " to value: ", Value)
    #     #     ###if self.M.getStateRep(s) != None:
    #     #     self.M.Q_bu[n][self.M.getStateRep(s)][a] = (1.0 - self.alpha)*self.M.Q[n][self.M.getStateRep(s)][a] + (self.alpha)*Value
    #
    #
    # ################################################################
    # # Q Backup
    # ################################################################
    # def QBackup(self, n, WW=None):
    #
    #     for s in self.M.getStates():
    #         for a in self.M.getActions(s, n):
    #             self.M.Q[n][s][a] = self.M.Q_bu[n][s][a]
    #             print("Q: ", self.M.Q[n][s][a])
    #             self.M.QSums[n][self.M.getStateRep(s)][a] += self.M.Q[n][self.M.getStateRep(s)][a]
    #
    #
    # ################################################################
    # # Regret/Policy Update
    # ################################################################
    # # TODO:
    # # check that this is correct
    # def regretUpdate(self, n, currentState, t):
    #
    #     # Terminal: No updates, return
    #     if self.M.isTerminal(self.M.getStateRep(currentState)):
    #         return
    #
    #     if self.M.getStateRep(currentState) == None:
    #         return
    #
    #     # Make all of them based on DCFR
    #
    #     # RM+ is
    #     # alphaR = inf
    #     # betaR = -inf
    #     # gammaR = 2
    #
    #     alphaR = 3.0 / 2.0      # accum pos regrets
    #     betaR = 0.0             # accum neg regrets
    #     gammaR = 2.0            # contribution to avg strategy
    #
    #     alphaW = pow(t, self.alphaDCFR)
    #     alphaWeight = (alphaW / (alphaW + 1))
    #
    #     betaW = pow(t, self.betaDCFR)
    #     betaWeight = (betaW / (betaW + 1))
    #
    #     gammaWeight = pow((t / (t + 1)), self.gammaDCFR)
    #
    #
    #     target = 0.0
    #
    #
    #     for a in self.M.getActions(currentState, n):
    #         target += self.M.Q[n][self.M.getStateRep(currentState)][a] * self.M.pi[n][self.M.getStateRep(currentState)][a]
    #
    #     for a in self.M.getActions(currentState, n):
    #         action_regret = self.M.Q[n][self.M.getStateRep(currentState)][a] - target
    #
    #         if self.DCFR:
    #             if action_regret > 0:
    #                 action_regret *= alphaWeight
    #             else:
    #                 action_regret *= betaWeight
    #
    #         #RMPLUS = False
    #         if self.RMPLUS:
    #             self.M.regret_sums[n][self.M.getStateRep(currentState)][a] = max(0.0, self.M.regret_sums[n][self.M.getStateRep(currentState)][a] + action_regret)
    #         else:
    #             self.M.regret_sums[n][self.M.getStateRep(currentState)][a] += action_regret
    #
    #
    #
    #     for a in self.M.getActions(self.M.getStateRep(currentState), n):
    #
    #         if self.M.isTerminal(currentState):
    #             continue
    #         # Sum up total regret
    #         rgrt_sum = 0.0
    #         for k in self.M.regret_sums[n][self.M.getStateRep(currentState)].keys():
    #             rgrt_sum += self.M.regret_sums[n][self.M.getStateRep(currentState)][k] if self.M.regret_sums[n][self.M.getStateRep(currentState)][k] > 0 else 0.0
    #
    #         # Check if this is a "trick"
    #         # Check if this can go here or not
    #         if rgrt_sum > 0:
    #             self.M.pi[n][self.M.getStateRep(currentState)][a] = (max(self.M.regret_sums[n][self.M.getStateRep(currentState)][a], 0.)) / rgrt_sum
    #         else:
    #             self.M.pi[n][self.M.getStateRep(currentState)][a] = 1.0 / len(self.M.getActions(self.M.getStateRep(currentState), n))
    #
    #         # Add to policy sum
    #         if self.DCFR:
    #             self.M.pi_sums[n][self.M.getStateRep(currentState)][a] += self.M.pi[n][self.M.getStateRep(currentState)][a] * gammaWeight
    #         else:
    #             self.M.pi_sums[n][self.M.getStateRep(currentState)][a] += self.M.pi[n][self.M.getStateRep(currentState)][a]
    #

############################################################################