


# Online episodic Q learning LONR

    Setup:
        Action State -> action -> Action State -> action -> etc
        Episode simulates acting until terminal (Open left or Open right)
        Because of this, states at bottom can be visited more than once,
            But then policy/regrets update once/multiple times

    Backup Q, update regrets and policy after each action?
    How to deal with states being revisited during an episode?
    **These results now at line 120**



   GAMMA = 0.95
   Iterations: 100,000

       Agent pi:
            root :   {'TR': 0.5, 'TL': 0.5}
            rootTL :   {'OR': 0.0006522886226027822, 'L': 0.9993477113773973, 'OL': 0.0}
            rootTLLGL :   {'OR': 0.009502050194708733, 'L': 0.9904979498052913, 'OL': 0.0}
            rootTLLGLLGL :   {'OR': 0.9571113848581173, 'L': 0.04288861514188268, 'OL': 0.0}
            rootTLLGLLGR :   {'OR': 0.0, 'L': 0.9982324884241078, 'OL': 0.0017675115758920541}
            rootTLLGR :   {'OR': 0.0, 'L': 0.9968284744358648, 'OL': 0.003171525564135265}
            rootTLLGRLGL :   {'OR': 0.0, 'L': 0.9929810096436621, 'OL': 0.0070189903563379836}
            rootTLLGRLGR :   {'OR': 0.0, 'L': 0.034223487201066764, 'OL': 0.9657765127989332}

       Agent pi_sums:
            root :   {'TR': 0.0, 'TL': 0.0}
            rootTL :   {'OR': 147.025352550285, 'L': 99716.67399873909, 'OL': 136.30064871158405}
            rootTLLGL :   {'OR': 377.67000967268194, 'L': 46295.3373874912, 'OL': 45.99260283609689}
            rootTLLGLLGL :   {'OR': 39092.273143128776, 'L': 3228.324514012705, 'OL': 51.40234285851369}
            rootTLLGLLGR :   {'OR': 79.93685872468093, 'L': 17295.705893104652, 'OL': 52.35724817070178}
            rootTLLGR :   {'OR': 46.61253504557436, 'L': 45905.845542188996, 'OL': 448.5419227652598}
            rootTLLGRLGL :   {'OR': 50.397868979020814, 'L': 17389.750374869014, 'OL': 77.85175615192107}
            rootTLLGRLGR :   {'OR': 52.77742247017579, 'L': 3299.0742343760653, 'OL': 38404.14834315373}


    GAMMA = 0.99
    Iterations: 100,000

        Agent pi:
            root :   {'TL': 0.5, 'TR': 0.5}
            rootTL :   {'L': 0.9991320446768248, 'OR': 0.0008679553231752267, 'OL': 0.0}
            rootTLLGL :   {'L': 0.9971153865593728, 'OR': 0.0028846134406271817, 'OL': 0.0}
            rootTLLGLLGL :   {'L': 0.7165308421232899, 'OR': 0.2834691578767101, 'OL': 0.0}
            rootTLLGLLGR :   {'L': 0.9979739117281262, 'OR': 0.00202608827187381, 'OL': 0.0}
            rootTLLGR :   {'L': 0.9692979854662558, 'OR': 0.0, 'OL': 0.030702014533744174}
            rootTLLGRLGL :   {'L': 0.997640586763592, 'OR': 0.0023594132364080323, 'OL': 0.0}
            rootTLLGRLGR :   {'L': 0.6926254063710268, 'OR': 0.0, 'OL': 0.3073745936289733}

        Agent pi_sums:
            root :   {'TL': 0.0, 'TR': 0.0}
            rootTL :   {'L': 99699.10264304215, 'OR': 151.42591244291, 'OL': 149.47144451501853}
            rootTLLGL :   {'L': 46079.11827032518, 'OR': 661.416512974787, 'OL': 39.465216700516386}
            rootTLLGLLGL :   {'L': 25188.644784281314, 'OR': 18553.281468070458, 'OL': 39.07374764826455}
            rootTLLGLLGR :   {'L': 27453.81666988673, 'OR': 95.73709179402212, 'OL': 76.44623831931524}
            rootTLLGR :   {'L': 45658.610674146374, 'OR': 38.212703021323875, 'OL': 581.1766228316361}
            rootTLLGRLGL :   {'L': 27401.635409892977, 'OR': 65.45224997787784, 'OL': 97.91234012907961}
            rootTLLGRLGR :   {'L': 25305.783903074614, 'OR': 40.03333589012639, 'OL': 18220.18276103534}


    GAMMA = 1.0
    Iterations: 100,000

        Agent pi:
            root :   {'TL': 0.5, 'TR': 0.5}
            rootTL :   {'OL': 0.0, 'L': 0.9990897513882521, 'OR': 0.0009102486117479379}
            rootTLLGL :   {'OL': 0.0, 'L': 0.9972841028476107, 'OR': 0.002715897152389253}
            rootTLLGLLGL :   {'OL': 0.0, 'L': 0.8106962368927146, 'OR': 0.18930376310728536}
            rootTLLGLLGR :   {'OL': 0.0, 'L': 0.9980586752014866, 'OR': 0.0019413247985133589}
            rootTLLGR :   {'OL': 0.004261191357230047, 'L': 0.9957388086427699, 'OR': 0.0}
            rootTLLGRLGL :   {'OL': 0.0, 'L': 0.9986248500657146, 'OR': 0.0013751499342855248}
            rootTLLGRLGR :   {'OL': 0.1899468624715124, 'L': 0.8100531375284876, 'OR': 0.0}

        Agent pi_sums:
            root :   {'TL': 0.0, 'TR': 0.0}
            rootTL :   {'OL': 152.94826526808038, 'L': 99692.07408170067, 'OR': 154.97765303087584}
            rootTLLGL :   {'OL': 39.15455538758672, 'L': 45714.47835778709, 'OR': 599.3670868254763}
            rootTLLGLLGL :   {'OL': 40.46646494258512, 'L': 27125.322752826898, 'OR': 16764.21078223054}
            rootTLLGLLGR :   {'OL': 77.44162869670679, 'L': 29473.739489706826, 'OR': 97.81888159648987}
            rootTLLGR :   {'OL': 655.6402335681245, 'L': 45936.622313533284, 'OR': 39.73745289852607}
            rootTLLGRLGL :   {'OL': 100.45688172869632, 'L': 29527.48881779137, 'OR': 73.05430047964448}
            rootTLLGRLGR :   {'OL': 16896.712129449108, 'L': 27098.4626355978, 'OR': 39.8252349532095}



    Results when states are repeated in visited list: (had this originally before realizing)
    Not correct but better results.
    Hinting at batch learning?

    GAMMA = 1.0
    Iterations: 10,000

        Agent pi:
            root :   {'TL': 0.5, 'TR': 0.5}
            rootTL :   {'OL': 0.0, 'L': 0.9978994176786925, 'OR': 0.0021005823213075857}
            rootTLLGL :   {'OL': 0.0, 'L': 0.9978877941195128, 'OR': 0.0021122058804872936}
            rootTLLGLLGL :   {'OL': 0.0, 'L': 0.0, 'OR': 1.0}
            rootTLLGLLGR :   {'OL': 0.0024594088756623735, 'L': 0.9975405911243377, 'OR': 0.0}
            rootTLLGR :   {'OL': 0.002506937768120661, 'L': 0.9974930622318794, 'OR': 0.0}
            rootTLLGRLGL :   {'OL': 0.0, 'L': 0.9939023406423956, 'OR': 0.0060976593576045094}
            rootTLLGRLGR :   {'OL': 0.9914920456562376, 'L': 0.008507954343762334, 'OR': 0.0}

        Agent pi_sums:
            root :   {'TL': 0.0, 'TR': 0.0}
            rootTL :   {'OL': 47.97140139737612, 'L': 9904.760145789549, 'OR': 47.26845281306164}
            rootTLLGL :   {'OL': 16.201589561529385, 'L': 4466.361267381553, 'OR': 138.43714305691572}
            rootTLLGLLGL :   {'OL': 15.006734863987777, 'L': 282.0808141744826, 'OR': 4110.912450961529}
            rootTLLGLLGR :   {'OL': 23.30431340169313, 'L': 1811.4486853418646, 'OR': 26.24700125644392}
            rootTLLGR :   {'OL': 141.5496164854156, 'L': 4464.653350836972, 'OR': 16.79703267761908}
            rootTLLGRLGL :   {'OL': 25.93917974296582, 'L': 1826.423016967337, 'OR': 20.63780328970049}
            rootTLLGRLGR :   {'OL': 4093.962662557419, 'L': 311.7234994239283, 'OR': 14.313838018651106}



   Results when Q copy over, pi update, regret update after each action:

        GAMMA = 1.0
        Iterations: 10,000

        Regular regret matching

        Agent pi:
            root :   {'TL': 0.5, 'TR': 0.5}
            rootTL :   {'OR': 0.0, 'OL': 0.0, 'L': 1.0}
            rootTLLGL :   {'OR': 0.0, 'OL': 0.0, 'L': 1.0}
            rootTLLGLLGL :   {'OR': 0.7305833087340237, 'OL': 0.0, 'L': 0.26941669126597634}
            rootTLLGLLGR :   {'OR': 0.0, 'OL': 0.0, 'L': 1.0}
            rootTLLGR :   {'OR': 0.0, 'OL': 0.0, 'L': 1.0}
            rootTLLGRLGL :   {'OR': 0.0, 'OL': 0.0, 'L': 1.0}
            rootTLLGRLGR :   {'OR': 0.0, 'OL': 0.7654194516924535, 'L': 0.23458054830754643}


        Agent pi_sums:
            root :   {'TL': 0.0, 'TR': 0.0}
            rootTL :   {'OR': 2.0, 'OL': 5.184759538031648, 'L': 9992.815240461969}
            rootTLLGL :   {'OR': 1.1742808798646363, 'OL': 0.7335105680359395, 'L': 4648.0922085521}
            rootTLLGLLGL :   {'OR': 4361.968905120103, 'OL': 0.3333333333333333, 'L': 299.6977615465644}
            rootTLLGLLGR :   {'OR': 0.3333333333333333, 'OL': 3.2183020813660326, 'L': 2002.4483645853006}
            rootTLLGR :   {'OR': 0.3333333333333333, 'OL': 17.24537299169646, 'L': 4647.42129367497}
            rootTLLGRLGL :   {'OR': 0.7007641514675569, 'OL': 1.2456903496052845, 'L': 2059.053545498927}
            rootTLLGRLGR :   {'OR': 0.3333333333333333, 'OL': 4274.402190039455, 'L': 161.26447662721017}

        Agent regret sums:
            root :   {'TL': 0.0, 'TR': 0.0}
            rootTL :   {'OR': -464135.34160350676, 'L': 116.99825149432357, 'OL': -469580.3416034987}
            rootTLLGL :   {'OR': -44968.15748349402, 'L': 84.80306083989008, 'OL': -402496.75748349365}
            rootTLLGLLGL :   {'OR': 1018.9999427193743, 'L': 159.93862664708564, 'OL': -496970.80005729763}
            rootTLLGLLGR :   {'OR': -111607.9756211367, 'L': 105.41303613886103, 'OL': -74146.37562113705}
            rootTLLGR :   {'OR': -400033.7769552233, 'L': 133.98374609371302, 'OL': -45475.17695521884}
            rootTLLGRLGL :   {'OR': -67013.39629709016, 'L': 105.76038143475654, 'OL': -119938.7962970905}
            rootTLLGRLGR :   {'OR': -480752.8451188881, 'L': 306.0629583757681, 'OL': 901.9548811316342}


        Some example episodes:
            ['rootTR', 'rootTRLGR']
            ['rootTRLGRLGL']
            ['rootTRLGLLGR']
            ['rootTRLGRLGL']
            ['rootTRLGLLGR']
            ['rootTRLGRLGR']
            ['rootTRLGRLGL']
            ['rootTRLGLLGR']
            ['rootTRLGRLGR']
            ['rootTROL']

            ['rootTL', 'rootTLLGL']
            ['rootTLLGLLGL']
            ['rootTLLGLLGR']
            ['rootTLLGRLGL']
            ['rootTLLGLLGL']
            ['rootTLOR']

            ['rootTL', 'rootTLLGL']
            ['rootTLLGLLGL']
            ['rootTLOR']

            ['rootTR', 'rootTRLGR']
            ['rootTRLGRLGR']
            ['rootTROL']

            ['rootTR', 'rootTRLGR']
            ['rootTRLGRLGR']
            ['rootTROL']


        Regret Matching+

            Iterations: 50,000

            Agent pi:
                root :   {'TR': 0.5, 'TL': 0.5}
                rootTL :   {'OR': 0.0, 'L': 0.9991049749868194, 'OL': 0.0008950250131805418}
                rootTLLGL :   {'OR': 0.0072718744781517, 'L': 0.9927281255218483, 'OL': 0.0}
                rootTLLGLLGL :   {'OR': 0.8686321250814234, 'L': 0.13136787491857652, 'OL': 0.0}
                rootTLLGLLGR :   {'OR': 0.004939839018428157, 'L': 0.9950601609815719, 'OL': 0.0}
                rootTLLGR :   {'OR': 0.0, 'L': 0.9897117035665538, 'OL': 0.010288296433446281}
                rootTLLGRLGL :   {'OR': 0.0011606904261724938, 'L': 0.9988393095738275, 'OL': 0.0}
                rootTLLGRLGR :   {'OR': 0.0, 'L': 0.045442773400590546, 'OL': 0.9545572265994096}

            Agent pi_sums:
                root :   {'TR': 0.0, 'TL': 0.0}
                rootTL :   {'OR': 100.6582546437668, 'L': 49799.00300053855, 'OL': 100.33874481744515}
                rootTLLGL :   {'OR': 245.7756713996856, 'L': 22950.34469819307, 'OL': 33.87963040721966}
                rootTLLGLLGL :   {'OR': 21354.397458831296, 'L': 2722.3726821159826, 'OL': 40.22985905275448}
                rootTLLGLLGR :   {'OR': 66.38583781491114, 'L': 10036.621598223137, 'OL': 42.992563961947816}
                rootTLLGR :   {'OR': 32.838020890839715, 'L': 23000.88640910276, 'OL': 282.2755700062763}
                rootTLLGRLGL :   {'OR': 45.219703246016415, 'L': 10040.134724007492, 'OL': 72.64557274647915}
                rootTLLGRLGR :   {'OR': 37.334290163166145, 'L': 3061.5558046783262, 'OL': 21226.109905158475}

           Regret sums:
                Agent regret sums:
                    root :   {'TR': 0.0, 'TL': 0.0}
                    rootTL :   {'OR': 0.0, 'L': 8803.167556448983, 'OL': 7.886113427015227}
                    rootTLLGL :   {'OR': 37.991612491697374, 'L': 5186.467721321466, 'OL': 0.0}
                    rootTLLGLLGL :   {'OR': 5443.542190532986, 'L': 823.2559549106004, 'OL': 0.0}
                    rootTLLGLLGR :   {'OR': 21.731949097815402, 'L': 4377.591392562506, 'OL': 0.0}
                    rootTLLGR :   {'OR': 0.0, 'L': 5205.06725069357, 'OL': 54.107953496134044}
                    rootTLLGRLGL :   {'OR': 5.14324831251448, 'L': 4426.054077467897, 'OL': 0.0}
                    rootTLLGRLGR :   {'OR': 0.0, 'L': 261.2349356723502, 'OL': 5487.422466231632}


# Value Iteration

